{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN\n",
    "\n",
    "* 深度Q网络定义\n",
    "* Replay buffer实现\n",
    "* 目标网络实现\n",
    "* Q值迭代\n",
    "* 目标网络更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (1.2.0)\n",
      "\u001b[33mWARNING: gymnasium 1.2.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (2.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: ale_py>=0.9 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.11.1)\n",
      "Requirement already satisfied: matplotlib in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: numpy in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (2.0.1)\n",
      "Requirement already satisfied: torch in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aaronwu/anaconda3/envs/pytorch/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "# 如果用zsh console请用这个命令 pip install gymnasium\\[all\\]\n",
    "! pip install matplotlib\n",
    "! pip install numpy\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.引入包并注册ale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from replaybuffer import ReplayBuffer\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.深度Q网络定义\n",
    "\n",
    "三层感知机网络：\n",
    "\n",
    "1. 状态空间->128；Relu\n",
    "2. 128 -> 128；Relu\n",
    "3. 128 -> 行为空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, env.action_space.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 定义经验回放缓存\n",
    "\n",
    "1. 经验回放缓存中存储(状态，行为，下时刻状态，奖励，回合是否结束)\n",
    "2. 当经验回放缓存已满，则覆盖最早插入的数据\n",
    "3. 实现随机采样的功能，并以tensor的形式返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6), device=\"cpu\"):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, 1))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = device\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.tensor(self.state[ind], device=self.device, dtype=torch.float32),\n",
    "\t\t\ttorch.tensor(self.action[ind], device=self.device, dtype=torch.int64),\n",
    "\t\t\ttorch.tensor(self.next_state[ind], device=self.device, dtype=torch.float32),\n",
    "\t\t\ttorch.tensor(self.reward[ind], device=self.device, dtype=torch.float32),\n",
    "\t\t\ttorch.tensor(self.not_done[ind], device=self.device, dtype=torch.float32)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 定义超参\n",
    "* learning_rate:深度学习学习率\n",
    "* buffer_size:经验回放缓存容量\n",
    "* total_timesteps:总训练步数\n",
    "* epsilon:探索与利用的探索值\n",
    "* gamma:折扣因子\n",
    "* tau:目标网络更新幅度\n",
    "* learning_start:从该时间步后开始训练\n",
    "* train_frequentce: N轮回合后训练一次\n",
    "* log_frequence:输出结果的频率\n",
    "* target_frequence:更新目标网络的频率\n",
    "* batch_size:每次训练网络的数据量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "buffer_size = int(1e5)\n",
    "total_timesteps = int(2e6)\n",
    "epsilon = 0.01\n",
    "gamma = 0.99\n",
    "tau = 1.0\n",
    "\n",
    "learning_starts = 10000\n",
    "train_frequency = 4\n",
    "log_frequency = 500\n",
    "target_frequency = 1000\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 构建环境和网络\n",
    "\n",
    "* 定义训练卡\n",
    "* 创建环境\n",
    "* 定义Q值网络\n",
    "* 定义目标网络\n",
    "* 定义优化器\n",
    "* 实例化缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.1+2750686)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"ALE/Boxing-v5\", obs_type=\"ram\")\n",
    "\n",
    "q_network = QNetwork(env).to(device)\n",
    "target_network = copy.deepcopy(q_network)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "buffer = ReplayBuffer(env.observation_space.shape[0], env.action_space.n, buffer_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 开始训练\n",
    "1. 初始化环境\n",
    "2. 根据网络选择行为\n",
    "3. 应用行为后环境推进\n",
    "4. 将转移信息存入经验回放缓存\n",
    "5. 每隔一段时间训练Q值网络\n",
    "6. 每隔一段时间更新目标网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td_loss: 1.5051190853118896\t q_values: 10.80508041381836\t step: 10500, avg_rewards: -11.0\n",
      "td_loss: 1.134813666343689\t q_values: 10.915023803710938\t step: 11000, avg_rewards: -13.666666666666666\n",
      "td_loss: 0.5540050268173218\t q_values: 11.329310417175293\t step: 11500, avg_rewards: -13.666666666666666\n",
      "td_loss: 0.5722718834877014\t q_values: 11.132790565490723\t step: 12000, avg_rewards: -13.666666666666666\n",
      "td_loss: 0.3582562506198883\t q_values: 11.981813430786133\t step: 12500, avg_rewards: -13.666666666666666\n",
      "td_loss: 0.3659170866012573\t q_values: 11.944762229919434\t step: 13000, avg_rewards: -12.857142857142858\n",
      "td_loss: 0.331169456243515\t q_values: 12.395763397216797\t step: 13500, avg_rewards: -12.857142857142858\n",
      "td_loss: 0.3016524314880371\t q_values: 12.254671096801758\t step: 14000, avg_rewards: -12.857142857142858\n",
      "td_loss: 0.2946300506591797\t q_values: 12.326803207397461\t step: 14500, avg_rewards: -13.625\n",
      "td_loss: 0.1768348067998886\t q_values: 12.378885269165039\t step: 15000, avg_rewards: -13.625\n",
      "td_loss: 0.1453806459903717\t q_values: 12.746330261230469\t step: 15500, avg_rewards: -13.625\n",
      "td_loss: 0.17217636108398438\t q_values: 12.672150611877441\t step: 16000, avg_rewards: -13.625\n",
      "td_loss: 0.17970392107963562\t q_values: 12.769021034240723\t step: 16500, avg_rewards: -12.555555555555555\n",
      "td_loss: 0.14265389740467072\t q_values: 12.871459007263184\t step: 17000, avg_rewards: -12.555555555555555\n",
      "td_loss: 0.1444595754146576\t q_values: 12.973278999328613\t step: 17500, avg_rewards: -12.555555555555555\n",
      "td_loss: 0.14477109909057617\t q_values: 13.052530288696289\t step: 18000, avg_rewards: -12.0\n",
      "td_loss: 0.17297452688217163\t q_values: 13.107195854187012\t step: 18500, avg_rewards: -12.0\n",
      "td_loss: 0.1451045572757721\t q_values: 13.167665481567383\t step: 19000, avg_rewards: -12.0\n",
      "td_loss: 0.21474164724349976\t q_values: 13.102375030517578\t step: 19500, avg_rewards: -12.0\n",
      "td_loss: 0.1416797935962677\t q_values: 13.087345123291016\t step: 20000, avg_rewards: -10.363636363636363\n",
      "td_loss: 0.1621266007423401\t q_values: 13.273448944091797\t step: 20500, avg_rewards: -10.363636363636363\n",
      "td_loss: 0.12827399373054504\t q_values: 13.234375953674316\t step: 21000, avg_rewards: -10.363636363636363\n",
      "td_loss: 0.1589033603668213\t q_values: 13.309854507446289\t step: 21500, avg_rewards: -10.416666666666666\n",
      "td_loss: 0.1357210874557495\t q_values: 13.360893249511719\t step: 22000, avg_rewards: -10.416666666666666\n",
      "td_loss: 0.1513494998216629\t q_values: 13.50146770477295\t step: 22500, avg_rewards: -10.416666666666666\n",
      "td_loss: 0.11606691777706146\t q_values: 13.391345977783203\t step: 23000, avg_rewards: -10.416666666666666\n",
      "td_loss: 0.202666774392128\t q_values: 13.619711875915527\t step: 23500, avg_rewards: -10.846153846153847\n",
      "td_loss: 0.11372111737728119\t q_values: 13.479161262512207\t step: 24000, avg_rewards: -10.846153846153847\n",
      "td_loss: 0.16320732235908508\t q_values: 13.433258056640625\t step: 24500, avg_rewards: -10.846153846153847\n",
      "td_loss: 0.11106434464454651\t q_values: 13.586228370666504\t step: 25000, avg_rewards: -10.846153846153847\n",
      "td_loss: 0.10819035023450851\t q_values: 13.545308113098145\t step: 25500, avg_rewards: -12.0\n",
      "td_loss: 0.8369837403297424\t q_values: 13.583839416503906\t step: 26000, avg_rewards: -12.0\n",
      "td_loss: 0.11645342409610748\t q_values: 13.62579345703125\t step: 26500, avg_rewards: -12.0\n",
      "td_loss: 0.11776390671730042\t q_values: 13.793944358825684\t step: 27000, avg_rewards: -12.533333333333333\n",
      "td_loss: 0.12934377789497375\t q_values: 13.840160369873047\t step: 27500, avg_rewards: -12.533333333333333\n",
      "td_loss: 0.12204961478710175\t q_values: 13.92391586303711\t step: 28000, avg_rewards: -12.533333333333333\n",
      "td_loss: 0.12261255085468292\t q_values: 13.95253849029541\t step: 28500, avg_rewards: -12.533333333333333\n",
      "td_loss: 0.09395089745521545\t q_values: 13.894076347351074\t step: 29000, avg_rewards: -11.75\n",
      "td_loss: 0.0908491313457489\t q_values: 14.116280555725098\t step: 29500, avg_rewards: -11.75\n",
      "td_loss: 0.08510567992925644\t q_values: 14.260910987854004\t step: 30000, avg_rewards: -11.75\n",
      "td_loss: 0.12970784306526184\t q_values: 14.140331268310547\t step: 30500, avg_rewards: -11.0\n",
      "td_loss: 0.10747001320123672\t q_values: 14.230859756469727\t step: 31000, avg_rewards: -11.0\n",
      "td_loss: 0.08915257453918457\t q_values: 14.399609565734863\t step: 31500, avg_rewards: -11.0\n",
      "td_loss: 0.8176662921905518\t q_values: 14.342815399169922\t step: 32000, avg_rewards: -11.0\n",
      "td_loss: 0.09924590587615967\t q_values: 14.498350143432617\t step: 32500, avg_rewards: -10.38888888888889\n",
      "td_loss: 0.09967412054538727\t q_values: 14.366687774658203\t step: 33000, avg_rewards: -10.38888888888889\n",
      "td_loss: 0.0892338678240776\t q_values: 14.443455696105957\t step: 33500, avg_rewards: -10.38888888888889\n",
      "td_loss: 0.07646546512842178\t q_values: 14.350545883178711\t step: 34000, avg_rewards: -10.263157894736842\n",
      "td_loss: 0.6359983086585999\t q_values: 14.425551414489746\t step: 34500, avg_rewards: -10.263157894736842\n",
      "td_loss: 0.07886240631341934\t q_values: 14.533014297485352\t step: 35000, avg_rewards: -10.263157894736842\n",
      "td_loss: 0.4033930003643036\t q_values: 14.417268753051758\t step: 35500, avg_rewards: -10.263157894736842\n",
      "td_loss: 0.11295221745967865\t q_values: 14.573772430419922\t step: 36000, avg_rewards: -9.85\n",
      "td_loss: 0.08786982297897339\t q_values: 14.545620918273926\t step: 36500, avg_rewards: -9.85\n",
      "td_loss: 0.07784703373908997\t q_values: 14.544775009155273\t step: 37000, avg_rewards: -9.85\n",
      "td_loss: 0.1083110123872757\t q_values: 14.62742805480957\t step: 37500, avg_rewards: -9.85\n",
      "td_loss: 0.099942646920681\t q_values: 14.54497241973877\t step: 38000, avg_rewards: -9.904761904761905\n",
      "td_loss: 0.08738146722316742\t q_values: 14.706228256225586\t step: 38500, avg_rewards: -9.904761904761905\n",
      "td_loss: 0.05505431443452835\t q_values: 14.697145462036133\t step: 39000, avg_rewards: -9.904761904761905\n",
      "td_loss: 0.08806787431240082\t q_values: 14.708561897277832\t step: 39500, avg_rewards: -9.590909090909092\n",
      "td_loss: 0.0755724087357521\t q_values: 14.735804557800293\t step: 40000, avg_rewards: -9.590909090909092\n",
      "td_loss: 0.12855619192123413\t q_values: 14.90165901184082\t step: 40500, avg_rewards: -9.590909090909092\n",
      "td_loss: 0.0867956131696701\t q_values: 14.806995391845703\t step: 41000, avg_rewards: -9.590909090909092\n",
      "td_loss: 0.1367611587047577\t q_values: 14.91092300415039\t step: 41500, avg_rewards: -9.434782608695652\n",
      "td_loss: 0.13006716966629028\t q_values: 14.697131156921387\t step: 42000, avg_rewards: -9.434782608695652\n",
      "td_loss: 0.07456657290458679\t q_values: 14.839823722839355\t step: 42500, avg_rewards: -9.434782608695652\n",
      "td_loss: 0.07342664897441864\t q_values: 14.81399917602539\t step: 43000, avg_rewards: -10.0\n",
      "td_loss: 0.1379261314868927\t q_values: 14.701665878295898\t step: 43500, avg_rewards: -10.0\n",
      "td_loss: 0.04903462529182434\t q_values: 14.943880081176758\t step: 44000, avg_rewards: -10.0\n",
      "td_loss: 0.8996630907058716\t q_values: 14.890254974365234\t step: 44500, avg_rewards: -10.0\n",
      "td_loss: 0.08970919251441956\t q_values: 14.70414924621582\t step: 45000, avg_rewards: -10.8\n",
      "td_loss: 0.4339527189731598\t q_values: 14.953852653503418\t step: 45500, avg_rewards: -10.8\n",
      "td_loss: 0.11799205839633942\t q_values: 14.878171920776367\t step: 46000, avg_rewards: -10.8\n",
      "td_loss: 0.09103035181760788\t q_values: 14.88254165649414\t step: 46500, avg_rewards: -10.153846153846153\n",
      "td_loss: 0.07797272503376007\t q_values: 15.121885299682617\t step: 47000, avg_rewards: -10.153846153846153\n",
      "td_loss: 0.06072975695133209\t q_values: 15.194199562072754\t step: 47500, avg_rewards: -10.153846153846153\n",
      "td_loss: 0.07343178242444992\t q_values: 15.133023262023926\t step: 48000, avg_rewards: -10.153846153846153\n",
      "td_loss: 0.07182140648365021\t q_values: 15.298917770385742\t step: 48500, avg_rewards: -10.0\n",
      "td_loss: 0.06762988120317459\t q_values: 15.192512512207031\t step: 49000, avg_rewards: -10.0\n",
      "td_loss: 0.7077520489692688\t q_values: 15.27227783203125\t step: 49500, avg_rewards: -10.0\n",
      "td_loss: 0.1166355237364769\t q_values: 15.332250595092773\t step: 50000, avg_rewards: -10.0\n",
      "td_loss: 0.08124861121177673\t q_values: 15.36970329284668\t step: 50500, avg_rewards: -9.642857142857142\n",
      "td_loss: 0.08749853819608688\t q_values: 15.481880187988281\t step: 51000, avg_rewards: -9.642857142857142\n",
      "td_loss: 0.06633961945772171\t q_values: 15.454504013061523\t step: 51500, avg_rewards: -9.642857142857142\n",
      "td_loss: 0.07169021666049957\t q_values: 15.618534088134766\t step: 52000, avg_rewards: -9.793103448275861\n",
      "td_loss: 0.09048417210578918\t q_values: 15.571662902832031\t step: 52500, avg_rewards: -9.793103448275861\n",
      "td_loss: 0.045822225511074066\t q_values: 15.752799987792969\t step: 53000, avg_rewards: -9.793103448275861\n",
      "td_loss: 0.08390332758426666\t q_values: 15.668403625488281\t step: 53500, avg_rewards: -9.793103448275861\n",
      "td_loss: 0.08519308269023895\t q_values: 15.678369522094727\t step: 54000, avg_rewards: -9.7\n",
      "td_loss: 0.09386417269706726\t q_values: 15.753711700439453\t step: 54500, avg_rewards: -9.7\n",
      "td_loss: 0.05164239555597305\t q_values: 15.793163299560547\t step: 55000, avg_rewards: -9.7\n",
      "td_loss: 0.11446133255958557\t q_values: 15.8316068649292\t step: 55500, avg_rewards: -9.548387096774194\n",
      "td_loss: 0.12839511036872864\t q_values: 15.630334854125977\t step: 56000, avg_rewards: -9.548387096774194\n",
      "td_loss: 0.06253319978713989\t q_values: 15.850835800170898\t step: 56500, avg_rewards: -9.548387096774194\n",
      "td_loss: 0.07808852940797806\t q_values: 15.856546401977539\t step: 57000, avg_rewards: -9.548387096774194\n",
      "td_loss: 0.08905661106109619\t q_values: 15.901344299316406\t step: 57500, avg_rewards: -9.0\n",
      "td_loss: 0.08144830167293549\t q_values: 15.807208061218262\t step: 58000, avg_rewards: -9.0\n",
      "td_loss: 0.14297989010810852\t q_values: 15.93337631225586\t step: 58500, avg_rewards: -9.0\n",
      "td_loss: 0.062024664133787155\t q_values: 16.01434326171875\t step: 59000, avg_rewards: -9.06060606060606\n",
      "td_loss: 0.08017899096012115\t q_values: 16.024879455566406\t step: 59500, avg_rewards: -9.06060606060606\n",
      "td_loss: 0.08435571193695068\t q_values: 15.862617492675781\t step: 60000, avg_rewards: -9.06060606060606\n",
      "td_loss: 0.6072717905044556\t q_values: 15.980569839477539\t step: 60500, avg_rewards: -9.06060606060606\n",
      "td_loss: 0.10468733310699463\t q_values: 15.975286483764648\t step: 61000, avg_rewards: -8.941176470588236\n",
      "td_loss: 0.0953746885061264\t q_values: 15.946313858032227\t step: 61500, avg_rewards: -8.941176470588236\n",
      "td_loss: 0.07847540080547333\t q_values: 15.924257278442383\t step: 62000, avg_rewards: -8.941176470588236\n",
      "td_loss: 0.08543239533901215\t q_values: 16.001554489135742\t step: 62500, avg_rewards: -8.941176470588236\n",
      "td_loss: 0.1142658069729805\t q_values: 16.075599670410156\t step: 63000, avg_rewards: -8.942857142857143\n",
      "td_loss: 0.12229500710964203\t q_values: 16.069740295410156\t step: 63500, avg_rewards: -8.942857142857143\n",
      "td_loss: 0.12178006768226624\t q_values: 16.09183692932129\t step: 64000, avg_rewards: -8.942857142857143\n",
      "td_loss: 0.07267934083938599\t q_values: 16.185415267944336\t step: 64500, avg_rewards: -8.972222222222221\n",
      "td_loss: 0.10588254779577255\t q_values: 15.984477043151855\t step: 65000, avg_rewards: -8.972222222222221\n",
      "td_loss: 0.2768312096595764\t q_values: 16.078880310058594\t step: 65500, avg_rewards: -8.972222222222221\n",
      "td_loss: 0.11482877284288406\t q_values: 16.109390258789062\t step: 66000, avg_rewards: -8.972222222222221\n",
      "td_loss: 0.13665339350700378\t q_values: 16.3565731048584\t step: 66500, avg_rewards: -8.945945945945946\n",
      "td_loss: 0.09369882941246033\t q_values: 16.341835021972656\t step: 67000, avg_rewards: -8.945945945945946\n",
      "td_loss: 0.05860542505979538\t q_values: 16.368722915649414\t step: 67500, avg_rewards: -8.945945945945946\n",
      "td_loss: 0.08422438055276871\t q_values: 16.259511947631836\t step: 68000, avg_rewards: -8.710526315789474\n",
      "td_loss: 0.07036522030830383\t q_values: 16.236825942993164\t step: 68500, avg_rewards: -8.710526315789474\n",
      "td_loss: 0.09545349329710007\t q_values: 16.282196044921875\t step: 69000, avg_rewards: -8.710526315789474\n",
      "td_loss: 0.09232950955629349\t q_values: 16.341707229614258\t step: 69500, avg_rewards: -8.710526315789474\n",
      "td_loss: 0.11715671420097351\t q_values: 16.2249755859375\t step: 70000, avg_rewards: -8.487179487179487\n",
      "td_loss: 0.08839051425457001\t q_values: 16.14179801940918\t step: 70500, avg_rewards: -8.487179487179487\n",
      "td_loss: 0.13045963644981384\t q_values: 16.429832458496094\t step: 71000, avg_rewards: -8.487179487179487\n",
      "td_loss: 0.08537812530994415\t q_values: 16.13490104675293\t step: 71500, avg_rewards: -8.225\n",
      "td_loss: 0.11931996792554855\t q_values: 16.137523651123047\t step: 72000, avg_rewards: -8.225\n",
      "td_loss: 0.08182719349861145\t q_values: 16.33494758605957\t step: 72500, avg_rewards: -8.225\n",
      "td_loss: 0.09992630779743195\t q_values: 16.51502799987793\t step: 73000, avg_rewards: -8.225\n",
      "td_loss: 0.051275841891765594\t q_values: 16.562288284301758\t step: 73500, avg_rewards: -8.170731707317072\n",
      "td_loss: 0.180100217461586\t q_values: 16.435588836669922\t step: 74000, avg_rewards: -8.170731707317072\n",
      "td_loss: 0.11635591834783554\t q_values: 16.514118194580078\t step: 74500, avg_rewards: -8.170731707317072\n",
      "td_loss: 0.12681299448013306\t q_values: 16.5602970123291\t step: 75000, avg_rewards: -8.170731707317072\n",
      "td_loss: 0.07234960794448853\t q_values: 16.477981567382812\t step: 75500, avg_rewards: -7.9523809523809526\n",
      "td_loss: 0.08353409171104431\t q_values: 16.57011604309082\t step: 76000, avg_rewards: -7.9523809523809526\n",
      "td_loss: 0.06904073804616928\t q_values: 16.554676055908203\t step: 76500, avg_rewards: -7.9523809523809526\n",
      "td_loss: 0.3049865961074829\t q_values: 16.503589630126953\t step: 77000, avg_rewards: -7.744186046511628\n",
      "td_loss: 0.13035404682159424\t q_values: 16.676755905151367\t step: 77500, avg_rewards: -7.744186046511628\n",
      "td_loss: 0.09417006373405457\t q_values: 16.79984474182129\t step: 78000, avg_rewards: -7.744186046511628\n",
      "td_loss: 0.08997845649719238\t q_values: 16.74631118774414\t step: 78500, avg_rewards: -7.744186046511628\n",
      "td_loss: 0.09640802443027496\t q_values: 16.745952606201172\t step: 79000, avg_rewards: -7.590909090909091\n",
      "td_loss: 0.07390490174293518\t q_values: 16.82445526123047\t step: 79500, avg_rewards: -7.590909090909091\n",
      "td_loss: 0.1204647421836853\t q_values: 16.934322357177734\t step: 80000, avg_rewards: -7.590909090909091\n",
      "td_loss: 0.09342397749423981\t q_values: 16.72568702697754\t step: 80500, avg_rewards: -7.4222222222222225\n",
      "td_loss: 0.29032206535339355\t q_values: 16.681421279907227\t step: 81000, avg_rewards: -7.4222222222222225\n",
      "td_loss: 0.09310757368803024\t q_values: 16.78076934814453\t step: 81500, avg_rewards: -7.4222222222222225\n",
      "td_loss: 0.06877653300762177\t q_values: 16.898937225341797\t step: 82000, avg_rewards: -7.4222222222222225\n",
      "td_loss: 0.14880114793777466\t q_values: 16.708763122558594\t step: 82500, avg_rewards: -7.173913043478261\n",
      "td_loss: 0.08861476182937622\t q_values: 16.879981994628906\t step: 83000, avg_rewards: -7.173913043478261\n",
      "td_loss: 0.05532921105623245\t q_values: 16.89057731628418\t step: 83500, avg_rewards: -7.173913043478261\n",
      "td_loss: 0.1522189825773239\t q_values: 16.629257202148438\t step: 84000, avg_rewards: -7.042553191489362\n",
      "td_loss: 0.19347600638866425\t q_values: 16.719257354736328\t step: 84500, avg_rewards: -7.042553191489362\n",
      "td_loss: 0.14519357681274414\t q_values: 16.81731605529785\t step: 85000, avg_rewards: -7.042553191489362\n",
      "td_loss: 0.09136250615119934\t q_values: 16.612991333007812\t step: 85500, avg_rewards: -7.042553191489362\n",
      "td_loss: 0.060730524361133575\t q_values: 16.735294342041016\t step: 86000, avg_rewards: -6.666666666666667\n",
      "td_loss: 0.1577121913433075\t q_values: 16.789592742919922\t step: 86500, avg_rewards: -6.666666666666667\n",
      "td_loss: 0.11144103109836578\t q_values: 16.686893463134766\t step: 87000, avg_rewards: -6.666666666666667\n",
      "td_loss: 0.11998264491558075\t q_values: 16.979848861694336\t step: 87500, avg_rewards: -6.666666666666667\n",
      "td_loss: 0.07966771721839905\t q_values: 16.652292251586914\t step: 88000, avg_rewards: -6.428571428571429\n",
      "td_loss: 0.308663547039032\t q_values: 16.745725631713867\t step: 88500, avg_rewards: -6.428571428571429\n",
      "td_loss: 0.09849543869495392\t q_values: 16.662696838378906\t step: 89000, avg_rewards: -6.428571428571429\n",
      "td_loss: 0.07190202921628952\t q_values: 16.477420806884766\t step: 89500, avg_rewards: -6.32\n",
      "td_loss: 0.10290782153606415\t q_values: 16.441913604736328\t step: 90000, avg_rewards: -6.32\n",
      "td_loss: 0.07416900247335434\t q_values: 16.65048599243164\t step: 90500, avg_rewards: -6.32\n",
      "td_loss: 0.08246271312236786\t q_values: 16.726520538330078\t step: 91000, avg_rewards: -6.32\n",
      "td_loss: 0.07116538286209106\t q_values: 16.673423767089844\t step: 91500, avg_rewards: -6.117647058823529\n",
      "td_loss: 0.3661947250366211\t q_values: 16.67981719970703\t step: 92000, avg_rewards: -6.117647058823529\n",
      "td_loss: 0.11103799939155579\t q_values: 16.602954864501953\t step: 92500, avg_rewards: -6.117647058823529\n",
      "td_loss: 0.07997556775808334\t q_values: 16.635202407836914\t step: 93000, avg_rewards: -6.1923076923076925\n",
      "td_loss: 0.0573013499379158\t q_values: 16.575855255126953\t step: 93500, avg_rewards: -6.1923076923076925\n",
      "td_loss: 0.24753671884536743\t q_values: 16.45669937133789\t step: 94000, avg_rewards: -6.1923076923076925\n",
      "td_loss: 0.08871591091156006\t q_values: 16.511220932006836\t step: 94500, avg_rewards: -6.1923076923076925\n",
      "td_loss: 0.07313276827335358\t q_values: 16.43255043029785\t step: 95000, avg_rewards: -6.0\n",
      "td_loss: 0.08930107951164246\t q_values: 16.36322784423828\t step: 95500, avg_rewards: -6.0\n",
      "td_loss: 0.0787917971611023\t q_values: 16.345666885375977\t step: 96000, avg_rewards: -6.0\n",
      "td_loss: 0.15977132320404053\t q_values: 16.3343563079834\t step: 96500, avg_rewards: -5.833333333333333\n",
      "td_loss: 0.12207785248756409\t q_values: 16.557451248168945\t step: 97000, avg_rewards: -5.833333333333333\n",
      "td_loss: 0.14997775852680206\t q_values: 16.419153213500977\t step: 97500, avg_rewards: -5.833333333333333\n",
      "td_loss: 0.0952158272266388\t q_values: 16.425254821777344\t step: 98000, avg_rewards: -5.833333333333333\n",
      "td_loss: 0.11347419768571854\t q_values: 16.25433349609375\t step: 98500, avg_rewards: -5.709090909090909\n",
      "td_loss: 0.0648082047700882\t q_values: 16.35649871826172\t step: 99000, avg_rewards: -5.709090909090909\n",
      "td_loss: 0.14659929275512695\t q_values: 16.2495059967041\t step: 99500, avg_rewards: -5.709090909090909\n",
      "td_loss: 0.0571993850171566\t q_values: 16.32407569885254\t step: 100000, avg_rewards: -5.709090909090909\n",
      "td_loss: 0.06495331227779388\t q_values: 16.50673484802246\t step: 100500, avg_rewards: -5.732142857142857\n",
      "td_loss: 0.0568605437874794\t q_values: 16.446399688720703\t step: 101000, avg_rewards: -5.732142857142857\n",
      "td_loss: 0.22019410133361816\t q_values: 16.22846794128418\t step: 101500, avg_rewards: -5.732142857142857\n",
      "td_loss: 0.09043503552675247\t q_values: 16.42548179626465\t step: 102000, avg_rewards: -5.684210526315789\n",
      "td_loss: 0.07609245181083679\t q_values: 16.34014892578125\t step: 102500, avg_rewards: -5.684210526315789\n",
      "td_loss: 0.3439441919326782\t q_values: 16.35272789001465\t step: 103000, avg_rewards: -5.684210526315789\n",
      "td_loss: 0.06860630959272385\t q_values: 16.176860809326172\t step: 103500, avg_rewards: -5.684210526315789\n",
      "td_loss: 0.22419601678848267\t q_values: 16.184005737304688\t step: 104000, avg_rewards: -5.448275862068965\n",
      "td_loss: 0.08924776315689087\t q_values: 16.351551055908203\t step: 104500, avg_rewards: -5.448275862068965\n",
      "td_loss: 0.05739261582493782\t q_values: 16.386653900146484\t step: 105000, avg_rewards: -5.448275862068965\n",
      "td_loss: 0.052781879901885986\t q_values: 16.299606323242188\t step: 105500, avg_rewards: -5.6440677966101696\n",
      "td_loss: 0.1625380516052246\t q_values: 16.239540100097656\t step: 106000, avg_rewards: -5.6440677966101696\n",
      "td_loss: 0.3128849267959595\t q_values: 16.243913650512695\t step: 106500, avg_rewards: -5.6440677966101696\n",
      "td_loss: 0.08681005239486694\t q_values: 16.32455062866211\t step: 107000, avg_rewards: -5.6440677966101696\n",
      "td_loss: 0.07572849094867706\t q_values: 16.207605361938477\t step: 107500, avg_rewards: -5.483333333333333\n",
      "td_loss: 0.10026173293590546\t q_values: 16.241622924804688\t step: 108000, avg_rewards: -5.483333333333333\n",
      "td_loss: 0.10781705379486084\t q_values: 16.31428337097168\t step: 108500, avg_rewards: -5.483333333333333\n",
      "td_loss: 0.14159530401229858\t q_values: 16.369142532348633\t step: 109000, avg_rewards: -5.442622950819672\n",
      "td_loss: 0.22552821040153503\t q_values: 16.027259826660156\t step: 109500, avg_rewards: -5.442622950819672\n",
      "td_loss: 0.1085580512881279\t q_values: 16.322267532348633\t step: 110000, avg_rewards: -5.442622950819672\n",
      "td_loss: 0.12000775337219238\t q_values: 16.152421951293945\t step: 110500, avg_rewards: -5.442622950819672\n",
      "td_loss: 0.15591740608215332\t q_values: 16.08536148071289\t step: 111000, avg_rewards: -5.290322580645161\n",
      "td_loss: 0.07063372433185577\t q_values: 16.419361114501953\t step: 111500, avg_rewards: -5.290322580645161\n",
      "td_loss: 0.08702036738395691\t q_values: 16.273588180541992\t step: 112000, avg_rewards: -5.290322580645161\n",
      "td_loss: 0.14004439115524292\t q_values: 16.241008758544922\t step: 112500, avg_rewards: -5.290322580645161\n",
      "td_loss: 0.094827800989151\t q_values: 16.328514099121094\t step: 113000, avg_rewards: -5.158730158730159\n",
      "td_loss: 0.11747606098651886\t q_values: 16.443376541137695\t step: 113500, avg_rewards: -5.158730158730159\n",
      "td_loss: 0.13107702136039734\t q_values: 16.199687957763672\t step: 114000, avg_rewards: -5.158730158730159\n",
      "td_loss: 0.10028816014528275\t q_values: 16.51395606994629\t step: 114500, avg_rewards: -5.0625\n",
      "td_loss: 0.08909101039171219\t q_values: 16.226703643798828\t step: 115000, avg_rewards: -5.0625\n",
      "td_loss: 0.12778492271900177\t q_values: 16.434080123901367\t step: 115500, avg_rewards: -5.0625\n",
      "td_loss: 0.10107193887233734\t q_values: 16.345355987548828\t step: 116000, avg_rewards: -5.0625\n",
      "td_loss: 0.08209163695573807\t q_values: 16.328998565673828\t step: 116500, avg_rewards: -4.861538461538461\n",
      "td_loss: 0.07698038965463638\t q_values: 16.272748947143555\t step: 117000, avg_rewards: -4.861538461538461\n",
      "td_loss: 0.2797733545303345\t q_values: 16.24867820739746\t step: 117500, avg_rewards: -4.861538461538461\n",
      "td_loss: 0.13414715230464935\t q_values: 16.22491455078125\t step: 118000, avg_rewards: -4.7272727272727275\n",
      "td_loss: 0.2988656461238861\t q_values: 16.277969360351562\t step: 118500, avg_rewards: -4.7272727272727275\n",
      "td_loss: 0.08589975535869598\t q_values: 16.24825668334961\t step: 119000, avg_rewards: -4.7272727272727275\n",
      "td_loss: 0.10842035710811615\t q_values: 16.180789947509766\t step: 119500, avg_rewards: -4.7272727272727275\n",
      "td_loss: 0.0744180828332901\t q_values: 16.16298484802246\t step: 120000, avg_rewards: -4.656716417910448\n",
      "td_loss: 0.0784926488995552\t q_values: 16.144489288330078\t step: 120500, avg_rewards: -4.656716417910448\n",
      "td_loss: 0.323769211769104\t q_values: 16.038349151611328\t step: 121000, avg_rewards: -4.656716417910448\n",
      "td_loss: 0.08401376008987427\t q_values: 16.22397804260254\t step: 121500, avg_rewards: -4.588235294117647\n",
      "td_loss: 0.11741841584444046\t q_values: 15.901562690734863\t step: 122000, avg_rewards: -4.588235294117647\n",
      "td_loss: 0.1145980954170227\t q_values: 16.030715942382812\t step: 122500, avg_rewards: -4.588235294117647\n",
      "td_loss: 0.1176481693983078\t q_values: 16.14935302734375\t step: 123000, avg_rewards: -4.588235294117647\n",
      "td_loss: 0.08028062433004379\t q_values: 16.28099822998047\t step: 123500, avg_rewards: -4.434782608695652\n",
      "td_loss: 0.09008845686912537\t q_values: 16.304262161254883\t step: 124000, avg_rewards: -4.434782608695652\n",
      "td_loss: 0.07581190764904022\t q_values: 16.20439338684082\t step: 124500, avg_rewards: -4.434782608695652\n",
      "td_loss: 0.09096211940050125\t q_values: 16.207408905029297\t step: 125000, avg_rewards: -4.434782608695652\n",
      "td_loss: 0.04798681288957596\t q_values: 16.082454681396484\t step: 125500, avg_rewards: -4.328571428571428\n",
      "td_loss: 0.087325319647789\t q_values: 16.036975860595703\t step: 126000, avg_rewards: -4.328571428571428\n",
      "td_loss: 0.21208573877811432\t q_values: 15.961429595947266\t step: 126500, avg_rewards: -4.328571428571428\n",
      "td_loss: 0.18276558816432953\t q_values: 16.112672805786133\t step: 127000, avg_rewards: -4.154929577464789\n",
      "td_loss: 0.12367524206638336\t q_values: 16.020748138427734\t step: 127500, avg_rewards: -4.154929577464789\n",
      "td_loss: 0.07047748565673828\t q_values: 16.107986450195312\t step: 128000, avg_rewards: -4.154929577464789\n",
      "td_loss: 0.2352517545223236\t q_values: 16.019290924072266\t step: 128500, avg_rewards: -4.154929577464789\n",
      "td_loss: 0.046251825988292694\t q_values: 16.023469924926758\t step: 129000, avg_rewards: -4.097222222222222\n",
      "td_loss: 0.1237957626581192\t q_values: 15.730388641357422\t step: 129500, avg_rewards: -4.097222222222222\n",
      "td_loss: 0.19207042455673218\t q_values: 15.884086608886719\t step: 130000, avg_rewards: -4.097222222222222\n",
      "td_loss: 0.06574968993663788\t q_values: 16.183269500732422\t step: 130500, avg_rewards: -3.958904109589041\n",
      "td_loss: 0.08725902438163757\t q_values: 16.119277954101562\t step: 131000, avg_rewards: -3.958904109589041\n",
      "td_loss: 0.08546627312898636\t q_values: 15.85283374786377\t step: 131500, avg_rewards: -3.958904109589041\n",
      "td_loss: 0.10256511718034744\t q_values: 16.098148345947266\t step: 132000, avg_rewards: -3.958904109589041\n",
      "td_loss: 0.09603715687990189\t q_values: 16.146705627441406\t step: 132500, avg_rewards: -3.72972972972973\n",
      "td_loss: 0.0958392396569252\t q_values: 16.083419799804688\t step: 133000, avg_rewards: -3.72972972972973\n",
      "td_loss: 0.08027433604001999\t q_values: 16.193191528320312\t step: 133500, avg_rewards: -3.72972972972973\n",
      "td_loss: 0.08479772508144379\t q_values: 16.182971954345703\t step: 134000, avg_rewards: -3.6133333333333333\n",
      "td_loss: 0.18219581246376038\t q_values: 16.023141860961914\t step: 134500, avg_rewards: -3.6133333333333333\n",
      "td_loss: 0.12478992342948914\t q_values: 15.723430633544922\t step: 135000, avg_rewards: -3.6133333333333333\n",
      "td_loss: 0.08566359430551529\t q_values: 16.008121490478516\t step: 135500, avg_rewards: -3.6133333333333333\n",
      "td_loss: 0.058989718556404114\t q_values: 15.866964340209961\t step: 136000, avg_rewards: -3.513157894736842\n",
      "td_loss: 0.07757653295993805\t q_values: 15.906848907470703\t step: 136500, avg_rewards: -3.513157894736842\n",
      "td_loss: 0.07707647979259491\t q_values: 16.05517578125\t step: 137000, avg_rewards: -3.513157894736842\n",
      "td_loss: 0.07266657054424286\t q_values: 15.813464164733887\t step: 137500, avg_rewards: -3.513157894736842\n",
      "td_loss: 0.1535419225692749\t q_values: 15.804505348205566\t step: 138000, avg_rewards: -3.5194805194805197\n",
      "td_loss: 0.06677736341953278\t q_values: 15.723712921142578\t step: 138500, avg_rewards: -3.5194805194805197\n",
      "td_loss: 0.04859347268939018\t q_values: 15.750604629516602\t step: 139000, avg_rewards: -3.5194805194805197\n",
      "td_loss: 0.138643279671669\t q_values: 15.59325885772705\t step: 139500, avg_rewards: -3.5128205128205128\n",
      "td_loss: 0.16674023866653442\t q_values: 15.494330406188965\t step: 140000, avg_rewards: -3.5128205128205128\n",
      "td_loss: 0.08724105358123779\t q_values: 15.712348937988281\t step: 140500, avg_rewards: -3.5128205128205128\n",
      "td_loss: 0.13789023458957672\t q_values: 15.639641761779785\t step: 141000, avg_rewards: -3.5128205128205128\n",
      "td_loss: 0.10390809178352356\t q_values: 15.568001747131348\t step: 141500, avg_rewards: -3.5569620253164556\n",
      "td_loss: 0.12910929322242737\t q_values: 15.398778915405273\t step: 142000, avg_rewards: -3.5569620253164556\n",
      "td_loss: 0.10936424881219864\t q_values: 15.675475120544434\t step: 142500, avg_rewards: -3.5569620253164556\n",
      "td_loss: 0.09948070347309113\t q_values: 15.522830963134766\t step: 143000, avg_rewards: -3.525\n",
      "td_loss: 0.09170551598072052\t q_values: 15.453386306762695\t step: 143500, avg_rewards: -3.525\n",
      "td_loss: 0.07990314066410065\t q_values: 15.285197257995605\t step: 144000, avg_rewards: -3.525\n",
      "td_loss: 0.08323688805103302\t q_values: 15.457515716552734\t step: 144500, avg_rewards: -3.525\n",
      "td_loss: 0.42439037561416626\t q_values: 15.567975044250488\t step: 145000, avg_rewards: -3.493827160493827\n",
      "td_loss: 0.46367940306663513\t q_values: 15.492111206054688\t step: 145500, avg_rewards: -3.493827160493827\n",
      "td_loss: 0.07781538367271423\t q_values: 15.392431259155273\t step: 146000, avg_rewards: -3.493827160493827\n",
      "td_loss: 0.1198027953505516\t q_values: 15.528095245361328\t step: 146500, avg_rewards: -3.451219512195122\n",
      "td_loss: 0.0927603468298912\t q_values: 15.629744529724121\t step: 147000, avg_rewards: -3.451219512195122\n",
      "td_loss: 0.10518266260623932\t q_values: 15.420528411865234\t step: 147500, avg_rewards: -3.451219512195122\n",
      "td_loss: 0.1071634292602539\t q_values: 15.493485450744629\t step: 148000, avg_rewards: -3.451219512195122\n",
      "td_loss: 0.09498095512390137\t q_values: 15.295673370361328\t step: 148500, avg_rewards: -3.4096385542168677\n",
      "td_loss: 0.12123340368270874\t q_values: 15.265653610229492\t step: 149000, avg_rewards: -3.4096385542168677\n",
      "td_loss: 0.14918047189712524\t q_values: 15.366981506347656\t step: 149500, avg_rewards: -3.4096385542168677\n",
      "td_loss: 0.12177248299121857\t q_values: 14.994308471679688\t step: 150000, avg_rewards: -3.4096385542168677\n",
      "td_loss: 0.10204243659973145\t q_values: 15.090831756591797\t step: 150500, avg_rewards: -3.3095238095238093\n",
      "td_loss: 0.0840691477060318\t q_values: 15.148239135742188\t step: 151000, avg_rewards: -3.3095238095238093\n",
      "td_loss: 0.06941850483417511\t q_values: 15.029336929321289\t step: 151500, avg_rewards: -3.3095238095238093\n",
      "td_loss: 0.07158457487821579\t q_values: 15.018989562988281\t step: 152000, avg_rewards: -3.447058823529412\n",
      "td_loss: 0.07639865577220917\t q_values: 15.070253372192383\t step: 152500, avg_rewards: -3.447058823529412\n",
      "td_loss: 0.11855242401361465\t q_values: 15.06809139251709\t step: 153000, avg_rewards: -3.447058823529412\n",
      "td_loss: 0.1609598994255066\t q_values: 15.026801109313965\t step: 153500, avg_rewards: -3.447058823529412\n",
      "td_loss: 0.3430045247077942\t q_values: 15.150627136230469\t step: 154000, avg_rewards: -3.3255813953488373\n",
      "td_loss: 0.18407949805259705\t q_values: 15.004278182983398\t step: 154500, avg_rewards: -3.3255813953488373\n",
      "td_loss: 0.38871005177497864\t q_values: 14.910505294799805\t step: 155000, avg_rewards: -3.3255813953488373\n",
      "td_loss: 0.06191084533929825\t q_values: 14.759559631347656\t step: 155500, avg_rewards: -3.1724137931034484\n",
      "td_loss: 0.06609801203012466\t q_values: 14.73305892944336\t step: 156000, avg_rewards: -3.1724137931034484\n",
      "td_loss: 0.18997126817703247\t q_values: 14.812908172607422\t step: 156500, avg_rewards: -3.1724137931034484\n",
      "td_loss: 0.4645611643791199\t q_values: 14.890060424804688\t step: 157000, avg_rewards: -3.1724137931034484\n",
      "td_loss: 0.10272526741027832\t q_values: 14.910089492797852\t step: 157500, avg_rewards: -3.1818181818181817\n",
      "td_loss: 0.1279565691947937\t q_values: 14.96865463256836\t step: 158000, avg_rewards: -3.1818181818181817\n",
      "td_loss: 0.0698961615562439\t q_values: 15.00499153137207\t step: 158500, avg_rewards: -3.1818181818181817\n",
      "td_loss: 0.09534518420696259\t q_values: 14.945289611816406\t step: 159000, avg_rewards: -3.056179775280899\n",
      "td_loss: 0.21683330833911896\t q_values: 14.643643379211426\t step: 159500, avg_rewards: -3.056179775280899\n",
      "td_loss: 0.1535266637802124\t q_values: 15.006139755249023\t step: 160000, avg_rewards: -3.056179775280899\n",
      "td_loss: 0.07078014314174652\t q_values: 14.88157844543457\t step: 160500, avg_rewards: -3.056179775280899\n",
      "td_loss: 0.10509677976369858\t q_values: 14.71845817565918\t step: 161000, avg_rewards: -2.8333333333333335\n",
      "td_loss: 0.6099773645401001\t q_values: 15.004895210266113\t step: 161500, avg_rewards: -2.8333333333333335\n",
      "td_loss: 0.46114060282707214\t q_values: 14.82979965209961\t step: 162000, avg_rewards: -2.8333333333333335\n",
      "td_loss: 0.11596369743347168\t q_values: 14.826197624206543\t step: 162500, avg_rewards: -2.8333333333333335\n",
      "td_loss: 0.25900304317474365\t q_values: 14.756596565246582\t step: 163000, avg_rewards: -2.8131868131868134\n",
      "td_loss: 0.06986390799283981\t q_values: 14.784860610961914\t step: 163500, avg_rewards: -2.8131868131868134\n",
      "td_loss: 0.07048454880714417\t q_values: 14.784883499145508\t step: 164000, avg_rewards: -2.8131868131868134\n",
      "td_loss: 0.14078325033187866\t q_values: 14.656440734863281\t step: 164500, avg_rewards: -2.717391304347826\n",
      "td_loss: 0.07481255382299423\t q_values: 14.817249298095703\t step: 165000, avg_rewards: -2.717391304347826\n",
      "td_loss: 0.0640484094619751\t q_values: 14.868316650390625\t step: 165500, avg_rewards: -2.717391304347826\n",
      "td_loss: 0.12615419924259186\t q_values: 14.772361755371094\t step: 166000, avg_rewards: -2.717391304347826\n",
      "td_loss: 0.0586790032684803\t q_values: 14.99486255645752\t step: 166500, avg_rewards: -2.763440860215054\n",
      "td_loss: 0.2551787197589874\t q_values: 14.829012870788574\t step: 167000, avg_rewards: -2.763440860215054\n",
      "td_loss: 0.07797576487064362\t q_values: 14.671647071838379\t step: 167500, avg_rewards: -2.763440860215054\n",
      "td_loss: 0.0690586119890213\t q_values: 14.79224681854248\t step: 168000, avg_rewards: -2.74468085106383\n",
      "td_loss: 0.1191682368516922\t q_values: 14.732120513916016\t step: 168500, avg_rewards: -2.74468085106383\n",
      "td_loss: 0.06359099596738815\t q_values: 14.686836242675781\t step: 169000, avg_rewards: -2.74468085106383\n",
      "td_loss: 0.07060755789279938\t q_values: 14.910224914550781\t step: 169500, avg_rewards: -2.74468085106383\n",
      "td_loss: 0.06374562531709671\t q_values: 14.84668254852295\t step: 170000, avg_rewards: -2.4947368421052634\n",
      "td_loss: 0.14270776510238647\t q_values: 14.925603866577148\t step: 170500, avg_rewards: -2.4947368421052634\n",
      "td_loss: 0.07082752883434296\t q_values: 14.73363208770752\t step: 171000, avg_rewards: -2.4947368421052634\n",
      "td_loss: 0.11129392683506012\t q_values: 14.78795051574707\t step: 171500, avg_rewards: -2.5104166666666665\n",
      "td_loss: 0.06119939684867859\t q_values: 14.833824157714844\t step: 172000, avg_rewards: -2.5104166666666665\n",
      "td_loss: 0.08396396040916443\t q_values: 14.675963401794434\t step: 172500, avg_rewards: -2.5104166666666665\n",
      "td_loss: 0.08802767097949982\t q_values: 14.923843383789062\t step: 173000, avg_rewards: -2.5104166666666665\n",
      "td_loss: 0.0734298825263977\t q_values: 14.625221252441406\t step: 173500, avg_rewards: -2.422680412371134\n",
      "td_loss: 0.11621204018592834\t q_values: 14.861856460571289\t step: 174000, avg_rewards: -2.422680412371134\n",
      "td_loss: 0.09065432846546173\t q_values: 14.844100952148438\t step: 174500, avg_rewards: -2.422680412371134\n",
      "td_loss: 0.08579427748918533\t q_values: 14.882024765014648\t step: 175000, avg_rewards: -2.422680412371134\n",
      "td_loss: 0.18636903166770935\t q_values: 14.85595417022705\t step: 175500, avg_rewards: -2.2755102040816326\n",
      "td_loss: 0.068855419754982\t q_values: 14.779333114624023\t step: 176000, avg_rewards: -2.2755102040816326\n",
      "td_loss: 0.09308229386806488\t q_values: 14.56423568725586\t step: 176500, avg_rewards: -2.2755102040816326\n",
      "td_loss: 0.09853291511535645\t q_values: 14.548787117004395\t step: 177000, avg_rewards: -2.2626262626262625\n",
      "td_loss: 0.09253112971782684\t q_values: 14.331214904785156\t step: 177500, avg_rewards: -2.2626262626262625\n",
      "td_loss: 0.12805449962615967\t q_values: 14.400891304016113\t step: 178000, avg_rewards: -2.2626262626262625\n",
      "td_loss: 0.08070811629295349\t q_values: 14.356698989868164\t step: 178500, avg_rewards: -2.2626262626262625\n",
      "td_loss: 0.0835493952035904\t q_values: 14.52657413482666\t step: 179000, avg_rewards: -2.18\n",
      "td_loss: 0.19887599349021912\t q_values: 14.526605606079102\t step: 179500, avg_rewards: -2.18\n",
      "td_loss: 0.18507064878940582\t q_values: 14.412540435791016\t step: 180000, avg_rewards: -2.18\n",
      "td_loss: 0.06639181077480316\t q_values: 14.585394859313965\t step: 180500, avg_rewards: -2.1\n",
      "td_loss: 0.06559400260448456\t q_values: 14.456416130065918\t step: 181000, avg_rewards: -2.1\n",
      "td_loss: 0.0711347758769989\t q_values: 14.473140716552734\t step: 181500, avg_rewards: -2.1\n",
      "td_loss: 0.08780086785554886\t q_values: 14.587105751037598\t step: 182000, avg_rewards: -2.1\n",
      "td_loss: 0.13841107487678528\t q_values: 14.331489562988281\t step: 182500, avg_rewards: -1.87\n",
      "td_loss: 0.1317981481552124\t q_values: 14.417844772338867\t step: 183000, avg_rewards: -1.87\n",
      "td_loss: 0.17008577287197113\t q_values: 14.427722930908203\t step: 183500, avg_rewards: -1.87\n",
      "td_loss: 0.1385463923215866\t q_values: 14.499876022338867\t step: 184000, avg_rewards: -1.85\n",
      "td_loss: 0.11724775284528732\t q_values: 14.447490692138672\t step: 184500, avg_rewards: -1.85\n",
      "td_loss: 0.07283014059066772\t q_values: 14.429756164550781\t step: 185000, avg_rewards: -1.85\n",
      "td_loss: 0.08465377986431122\t q_values: 14.454055786132812\t step: 185500, avg_rewards: -1.85\n",
      "td_loss: 0.09710029512643814\t q_values: 14.34374713897705\t step: 186000, avg_rewards: -1.65\n",
      "td_loss: 0.06705162674188614\t q_values: 14.373786926269531\t step: 186500, avg_rewards: -1.65\n",
      "td_loss: 0.06904038041830063\t q_values: 14.447389602661133\t step: 187000, avg_rewards: -1.65\n",
      "td_loss: 0.103192038834095\t q_values: 14.53240966796875\t step: 187500, avg_rewards: -1.65\n",
      "td_loss: 0.09231045842170715\t q_values: 14.557950973510742\t step: 188000, avg_rewards: -1.6\n",
      "td_loss: 0.10544337332248688\t q_values: 14.573768615722656\t step: 188500, avg_rewards: -1.6\n",
      "td_loss: 0.11145351827144623\t q_values: 14.438106536865234\t step: 189000, avg_rewards: -1.6\n",
      "td_loss: 0.09140953421592712\t q_values: 14.561763763427734\t step: 189500, avg_rewards: -1.29\n",
      "td_loss: 0.06748709082603455\t q_values: 14.518431663513184\t step: 190000, avg_rewards: -1.29\n",
      "td_loss: 0.10166686773300171\t q_values: 14.604568481445312\t step: 190500, avg_rewards: -1.29\n",
      "td_loss: 0.08681511878967285\t q_values: 14.411909103393555\t step: 191000, avg_rewards: -1.29\n",
      "td_loss: 0.061740972101688385\t q_values: 14.552526473999023\t step: 191500, avg_rewards: -1.1\n",
      "td_loss: 0.061871737241744995\t q_values: 14.454789161682129\t step: 192000, avg_rewards: -1.1\n",
      "td_loss: 0.09871247410774231\t q_values: 14.408647537231445\t step: 192500, avg_rewards: -1.1\n",
      "td_loss: 0.1975337564945221\t q_values: 14.262161254882812\t step: 193000, avg_rewards: -0.91\n",
      "td_loss: 0.05995891988277435\t q_values: 14.31771183013916\t step: 193500, avg_rewards: -0.91\n",
      "td_loss: 0.17433834075927734\t q_values: 14.096120834350586\t step: 194000, avg_rewards: -0.91\n",
      "td_loss: 0.056730061769485474\t q_values: 14.160697937011719\t step: 194500, avg_rewards: -0.91\n",
      "td_loss: 0.06678524613380432\t q_values: 14.09658432006836\t step: 195000, avg_rewards: -0.79\n",
      "td_loss: 0.1908777356147766\t q_values: 14.057563781738281\t step: 195500, avg_rewards: -0.79\n",
      "td_loss: 0.06940547376871109\t q_values: 14.028752326965332\t step: 196000, avg_rewards: -0.79\n",
      "td_loss: 0.13833345472812653\t q_values: 14.047819137573242\t step: 196500, avg_rewards: -0.72\n",
      "td_loss: 0.08158017694950104\t q_values: 14.022638320922852\t step: 197000, avg_rewards: -0.72\n",
      "td_loss: 0.09454672038555145\t q_values: 14.075980186462402\t step: 197500, avg_rewards: -0.72\n",
      "td_loss: 0.056141674518585205\t q_values: 13.959367752075195\t step: 198000, avg_rewards: -0.72\n",
      "td_loss: 0.06842494755983353\t q_values: 14.129232406616211\t step: 198500, avg_rewards: -0.67\n",
      "td_loss: 0.06359930336475372\t q_values: 14.024928092956543\t step: 199000, avg_rewards: -0.67\n",
      "td_loss: 0.12256096303462982\t q_values: 14.17971420288086\t step: 199500, avg_rewards: -0.67\n",
      "td_loss: 0.12973468005657196\t q_values: 13.969974517822266\t step: 200000, avg_rewards: -0.67\n",
      "td_loss: 0.13073670864105225\t q_values: 14.090049743652344\t step: 200500, avg_rewards: -0.44\n",
      "td_loss: 0.07666058838367462\t q_values: 14.092398643493652\t step: 201000, avg_rewards: -0.44\n",
      "td_loss: 0.06507128477096558\t q_values: 14.011760711669922\t step: 201500, avg_rewards: -0.44\n",
      "td_loss: 0.06984605640172958\t q_values: 14.198848724365234\t step: 202000, avg_rewards: -0.3\n",
      "td_loss: 0.11708185821771622\t q_values: 13.999463081359863\t step: 202500, avg_rewards: -0.3\n",
      "td_loss: 0.10511305928230286\t q_values: 14.322078704833984\t step: 203000, avg_rewards: -0.3\n",
      "td_loss: 0.05848049744963646\t q_values: 14.139412879943848\t step: 203500, avg_rewards: -0.3\n",
      "td_loss: 0.08935663104057312\t q_values: 14.163503646850586\t step: 204000, avg_rewards: -0.05\n",
      "td_loss: 0.1584063172340393\t q_values: 14.068347930908203\t step: 204500, avg_rewards: -0.05\n",
      "td_loss: 0.10621713101863861\t q_values: 14.107446670532227\t step: 205000, avg_rewards: -0.05\n",
      "td_loss: 0.11148563027381897\t q_values: 13.99262809753418\t step: 205500, avg_rewards: 0.3\n",
      "td_loss: 0.06505587697029114\t q_values: 14.245871543884277\t step: 206000, avg_rewards: 0.3\n",
      "td_loss: 0.12039799988269806\t q_values: 13.9915189743042\t step: 206500, avg_rewards: 0.3\n",
      "td_loss: 0.11248774081468582\t q_values: 14.025867462158203\t step: 207000, avg_rewards: 0.3\n",
      "td_loss: 0.10317414999008179\t q_values: 13.93604850769043\t step: 207500, avg_rewards: 0.39\n",
      "td_loss: 0.12775632739067078\t q_values: 14.030424118041992\t step: 208000, avg_rewards: 0.39\n",
      "td_loss: 0.09077946841716766\t q_values: 14.148774147033691\t step: 208500, avg_rewards: 0.39\n",
      "td_loss: 0.10109609365463257\t q_values: 13.96973991394043\t step: 209000, avg_rewards: 0.37\n",
      "td_loss: 0.23976218700408936\t q_values: 14.10788345336914\t step: 209500, avg_rewards: 0.37\n",
      "td_loss: 0.09840865433216095\t q_values: 14.014286994934082\t step: 210000, avg_rewards: 0.37\n",
      "td_loss: 0.10927578806877136\t q_values: 14.053634643554688\t step: 210500, avg_rewards: 0.37\n",
      "td_loss: 0.0704309269785881\t q_values: 14.149023056030273\t step: 211000, avg_rewards: 0.45\n",
      "td_loss: 0.08092283457517624\t q_values: 14.188419342041016\t step: 211500, avg_rewards: 0.45\n",
      "td_loss: 0.08973239362239838\t q_values: 14.042279243469238\t step: 212000, avg_rewards: 0.45\n",
      "td_loss: 0.13627171516418457\t q_values: 14.156005859375\t step: 212500, avg_rewards: 0.45\n",
      "td_loss: 0.08889399468898773\t q_values: 14.082839965820312\t step: 213000, avg_rewards: 0.62\n",
      "td_loss: 0.10922002792358398\t q_values: 13.872335433959961\t step: 213500, avg_rewards: 0.62\n",
      "td_loss: 0.08591063320636749\t q_values: 13.821627616882324\t step: 214000, avg_rewards: 0.62\n",
      "td_loss: 0.06867348402738571\t q_values: 13.924205780029297\t step: 214500, avg_rewards: 0.64\n",
      "td_loss: 0.05411462113261223\t q_values: 13.800530433654785\t step: 215000, avg_rewards: 0.64\n",
      "td_loss: 0.12934212386608124\t q_values: 13.70974349975586\t step: 215500, avg_rewards: 0.64\n",
      "td_loss: 0.08508124947547913\t q_values: 13.845895767211914\t step: 216000, avg_rewards: 0.64\n",
      "td_loss: 0.08834640681743622\t q_values: 13.882230758666992\t step: 216500, avg_rewards: 0.76\n",
      "td_loss: 0.06313888728618622\t q_values: 13.813611030578613\t step: 217000, avg_rewards: 0.76\n",
      "td_loss: 0.09470926225185394\t q_values: 13.529396057128906\t step: 217500, avg_rewards: 0.76\n",
      "td_loss: 0.08676282316446304\t q_values: 13.642219543457031\t step: 218000, avg_rewards: 0.99\n",
      "td_loss: 0.07810254395008087\t q_values: 13.592886924743652\t step: 218500, avg_rewards: 0.99\n",
      "td_loss: 0.1390952169895172\t q_values: 13.518011093139648\t step: 219000, avg_rewards: 0.99\n",
      "td_loss: 0.07859686762094498\t q_values: 13.523067474365234\t step: 219500, avg_rewards: 0.99\n",
      "td_loss: 0.08404338359832764\t q_values: 13.494856834411621\t step: 220000, avg_rewards: 1.17\n",
      "td_loss: 0.0900033563375473\t q_values: 13.40102767944336\t step: 220500, avg_rewards: 1.17\n",
      "td_loss: 0.08919626474380493\t q_values: 13.521919250488281\t step: 221000, avg_rewards: 1.17\n",
      "td_loss: 0.06549711525440216\t q_values: 13.433343887329102\t step: 221500, avg_rewards: 1.48\n",
      "td_loss: 0.2275092750787735\t q_values: 13.60842514038086\t step: 222000, avg_rewards: 1.48\n",
      "td_loss: 0.1396082639694214\t q_values: 13.54271125793457\t step: 222500, avg_rewards: 1.48\n",
      "td_loss: 0.14938363432884216\t q_values: 13.809526443481445\t step: 223000, avg_rewards: 1.48\n",
      "td_loss: 0.1317664086818695\t q_values: 13.760562896728516\t step: 223500, avg_rewards: 1.89\n",
      "td_loss: 0.11887384206056595\t q_values: 13.650596618652344\t step: 224000, avg_rewards: 1.89\n",
      "td_loss: 0.1424873322248459\t q_values: 13.566686630249023\t step: 224500, avg_rewards: 1.89\n",
      "td_loss: 0.07796557247638702\t q_values: 13.602513313293457\t step: 225000, avg_rewards: 1.89\n",
      "td_loss: 0.09297870099544525\t q_values: 13.571866035461426\t step: 225500, avg_rewards: 1.91\n",
      "td_loss: 0.09552648663520813\t q_values: 13.662531852722168\t step: 226000, avg_rewards: 1.91\n",
      "td_loss: 0.12239886075258255\t q_values: 13.568771362304688\t step: 226500, avg_rewards: 1.91\n",
      "td_loss: 0.17817065119743347\t q_values: 13.367375373840332\t step: 227000, avg_rewards: 2.05\n",
      "td_loss: 0.1257692575454712\t q_values: 13.542181968688965\t step: 227500, avg_rewards: 2.05\n",
      "td_loss: 0.07090514898300171\t q_values: 13.516356468200684\t step: 228000, avg_rewards: 2.05\n",
      "td_loss: 0.07060565054416656\t q_values: 13.568719863891602\t step: 228500, avg_rewards: 2.05\n",
      "td_loss: 0.09269949793815613\t q_values: 13.564699172973633\t step: 229000, avg_rewards: 2.1\n",
      "td_loss: 0.11657755076885223\t q_values: 13.483646392822266\t step: 229500, avg_rewards: 2.1\n",
      "td_loss: 0.10260240733623505\t q_values: 13.446578979492188\t step: 230000, avg_rewards: 2.1\n",
      "td_loss: 0.10018463432788849\t q_values: 13.51546573638916\t step: 230500, avg_rewards: 2.34\n",
      "td_loss: 0.14165794849395752\t q_values: 13.489238739013672\t step: 231000, avg_rewards: 2.34\n",
      "td_loss: 0.18637199699878693\t q_values: 13.556381225585938\t step: 231500, avg_rewards: 2.34\n",
      "td_loss: 0.19906939566135406\t q_values: 13.54066276550293\t step: 232000, avg_rewards: 2.34\n",
      "td_loss: 0.1134946420788765\t q_values: 13.593210220336914\t step: 232500, avg_rewards: 2.52\n",
      "td_loss: 0.12326124310493469\t q_values: 13.591602325439453\t step: 233000, avg_rewards: 2.52\n",
      "td_loss: 0.10689599812030792\t q_values: 13.636125564575195\t step: 233500, avg_rewards: 2.52\n",
      "td_loss: 0.11676079034805298\t q_values: 13.617988586425781\t step: 234000, avg_rewards: 2.76\n",
      "td_loss: 0.13611075282096863\t q_values: 13.524723052978516\t step: 234500, avg_rewards: 2.76\n",
      "td_loss: 0.07239105552434921\t q_values: 13.429965019226074\t step: 235000, avg_rewards: 2.76\n",
      "td_loss: 0.06385435163974762\t q_values: 13.560140609741211\t step: 235500, avg_rewards: 2.76\n",
      "td_loss: 0.08146792650222778\t q_values: 13.484436988830566\t step: 236000, avg_rewards: 2.77\n",
      "td_loss: 0.10563014447689056\t q_values: 13.66413402557373\t step: 236500, avg_rewards: 2.77\n",
      "td_loss: 0.1703895926475525\t q_values: 13.50516414642334\t step: 237000, avg_rewards: 2.77\n",
      "td_loss: 0.09196491539478302\t q_values: 13.765780448913574\t step: 237500, avg_rewards: 2.77\n",
      "td_loss: 0.09383247047662735\t q_values: 13.472052574157715\t step: 238000, avg_rewards: 3.07\n",
      "td_loss: 0.13181674480438232\t q_values: 13.5690279006958\t step: 238500, avg_rewards: 3.07\n",
      "td_loss: 0.11484523117542267\t q_values: 13.590030670166016\t step: 239000, avg_rewards: 3.07\n",
      "td_loss: 0.1323070228099823\t q_values: 13.517465591430664\t step: 239500, avg_rewards: 3.37\n",
      "td_loss: 0.10554089397192001\t q_values: 13.594189643859863\t step: 240000, avg_rewards: 3.37\n",
      "td_loss: 0.09422482550144196\t q_values: 13.418798446655273\t step: 240500, avg_rewards: 3.37\n",
      "td_loss: 0.17535504698753357\t q_values: 13.45285701751709\t step: 241000, avg_rewards: 3.37\n",
      "td_loss: 0.09929411113262177\t q_values: 13.421104431152344\t step: 241500, avg_rewards: 3.53\n",
      "td_loss: 0.0756472647190094\t q_values: 13.465087890625\t step: 242000, avg_rewards: 3.53\n",
      "td_loss: 0.0779317170381546\t q_values: 13.640643119812012\t step: 242500, avg_rewards: 3.53\n",
      "td_loss: 0.11868435144424438\t q_values: 13.668453216552734\t step: 243000, avg_rewards: 3.7\n",
      "td_loss: 0.08750444650650024\t q_values: 13.443865776062012\t step: 243500, avg_rewards: 3.7\n",
      "td_loss: 0.10192711651325226\t q_values: 13.57149887084961\t step: 244000, avg_rewards: 3.7\n",
      "td_loss: 0.11434995383024216\t q_values: 13.434823989868164\t step: 244500, avg_rewards: 3.7\n",
      "td_loss: 0.08240893483161926\t q_values: 13.415599822998047\t step: 245000, avg_rewards: 3.78\n",
      "td_loss: 0.110944464802742\t q_values: 13.413257598876953\t step: 245500, avg_rewards: 3.78\n",
      "td_loss: 0.07532620429992676\t q_values: 13.329509735107422\t step: 246000, avg_rewards: 3.78\n",
      "td_loss: 0.12588989734649658\t q_values: 13.68853759765625\t step: 246500, avg_rewards: 3.81\n",
      "td_loss: 0.13327887654304504\t q_values: 13.642280578613281\t step: 247000, avg_rewards: 3.81\n",
      "td_loss: 0.08593755960464478\t q_values: 13.591379165649414\t step: 247500, avg_rewards: 3.81\n",
      "td_loss: 0.15800711512565613\t q_values: 13.51353645324707\t step: 248000, avg_rewards: 3.81\n",
      "td_loss: 0.0704355537891388\t q_values: 13.43885612487793\t step: 248500, avg_rewards: 3.85\n",
      "td_loss: 0.1262231469154358\t q_values: 13.500328063964844\t step: 249000, avg_rewards: 3.85\n",
      "td_loss: 0.15039139986038208\t q_values: 13.371865272521973\t step: 249500, avg_rewards: 3.85\n",
      "td_loss: 0.1321900337934494\t q_values: 13.457096099853516\t step: 250000, avg_rewards: 3.85\n",
      "td_loss: 0.07648098468780518\t q_values: 13.349029541015625\t step: 250500, avg_rewards: 4.06\n",
      "td_loss: 0.0917619913816452\t q_values: 13.365588188171387\t step: 251000, avg_rewards: 4.06\n",
      "td_loss: 0.08933541178703308\t q_values: 13.396041870117188\t step: 251500, avg_rewards: 4.06\n",
      "td_loss: 0.12340004742145538\t q_values: 13.424513816833496\t step: 252000, avg_rewards: 4.55\n",
      "td_loss: 0.09467081725597382\t q_values: 13.292774200439453\t step: 252500, avg_rewards: 4.55\n",
      "td_loss: 0.07573649287223816\t q_values: 13.537464141845703\t step: 253000, avg_rewards: 4.55\n",
      "td_loss: 0.10034307837486267\t q_values: 13.424562454223633\t step: 253500, avg_rewards: 4.55\n",
      "td_loss: 0.09308791905641556\t q_values: 13.363302230834961\t step: 254000, avg_rewards: 4.69\n",
      "td_loss: 0.13227398693561554\t q_values: 13.422621726989746\t step: 254500, avg_rewards: 4.69\n",
      "td_loss: 0.0903630405664444\t q_values: 13.558792114257812\t step: 255000, avg_rewards: 4.69\n",
      "td_loss: 0.11309646815061569\t q_values: 13.395952224731445\t step: 255500, avg_rewards: 4.88\n",
      "td_loss: 0.12735554575920105\t q_values: 13.161276817321777\t step: 256000, avg_rewards: 4.88\n",
      "td_loss: 0.08854667097330093\t q_values: 13.059568405151367\t step: 256500, avg_rewards: 4.88\n",
      "td_loss: 0.14213305711746216\t q_values: 13.288503646850586\t step: 257000, avg_rewards: 4.88\n",
      "td_loss: 0.13870200514793396\t q_values: 13.127403259277344\t step: 257500, avg_rewards: 5.08\n",
      "td_loss: 0.0786118358373642\t q_values: 13.186267852783203\t step: 258000, avg_rewards: 5.08\n",
      "td_loss: 0.1849604994058609\t q_values: 13.087300300598145\t step: 258500, avg_rewards: 5.08\n",
      "td_loss: 0.18358206748962402\t q_values: 13.214012145996094\t step: 259000, avg_rewards: 5.19\n",
      "td_loss: 0.09523977339267731\t q_values: 13.30439567565918\t step: 259500, avg_rewards: 5.19\n",
      "td_loss: 0.17813479900360107\t q_values: 13.143534660339355\t step: 260000, avg_rewards: 5.19\n",
      "td_loss: 0.1307784765958786\t q_values: 13.341529846191406\t step: 260500, avg_rewards: 5.19\n",
      "td_loss: 0.08520293235778809\t q_values: 13.203353881835938\t step: 261000, avg_rewards: 5.39\n",
      "td_loss: 0.15277498960494995\t q_values: 12.949623107910156\t step: 261500, avg_rewards: 5.39\n",
      "td_loss: 0.1360427290201187\t q_values: 13.14279842376709\t step: 262000, avg_rewards: 5.39\n",
      "td_loss: 0.07812826335430145\t q_values: 13.260936737060547\t step: 262500, avg_rewards: 5.39\n",
      "td_loss: 0.14837002754211426\t q_values: 13.17068862915039\t step: 263000, avg_rewards: 5.71\n",
      "td_loss: 0.0768202394247055\t q_values: 13.138031005859375\t step: 263500, avg_rewards: 5.71\n",
      "td_loss: 0.09424151480197906\t q_values: 13.228275299072266\t step: 264000, avg_rewards: 5.71\n",
      "td_loss: 0.09119629859924316\t q_values: 13.146682739257812\t step: 264500, avg_rewards: 5.76\n",
      "td_loss: 0.07636106014251709\t q_values: 13.22557258605957\t step: 265000, avg_rewards: 5.76\n",
      "td_loss: 0.0946827083826065\t q_values: 13.175859451293945\t step: 265500, avg_rewards: 5.76\n",
      "td_loss: 0.12725579738616943\t q_values: 13.21756362915039\t step: 266000, avg_rewards: 5.76\n",
      "td_loss: 0.10766150802373886\t q_values: 12.993093490600586\t step: 266500, avg_rewards: 5.78\n",
      "td_loss: 0.1850060224533081\t q_values: 13.140405654907227\t step: 267000, avg_rewards: 5.78\n",
      "td_loss: 0.13329924643039703\t q_values: 13.284320831298828\t step: 267500, avg_rewards: 5.78\n",
      "td_loss: 0.16481220722198486\t q_values: 13.018779754638672\t step: 268000, avg_rewards: 5.89\n",
      "td_loss: 0.11184945702552795\t q_values: 13.352344512939453\t step: 268500, avg_rewards: 5.89\n",
      "td_loss: 0.09017348289489746\t q_values: 13.123458862304688\t step: 269000, avg_rewards: 5.89\n",
      "td_loss: 0.11309055984020233\t q_values: 12.972244262695312\t step: 269500, avg_rewards: 5.89\n",
      "td_loss: 0.1456352025270462\t q_values: 12.938024520874023\t step: 270000, avg_rewards: 6.15\n",
      "td_loss: 0.07597793638706207\t q_values: 12.872369766235352\t step: 270500, avg_rewards: 6.15\n",
      "td_loss: 0.17315225303173065\t q_values: 12.948076248168945\t step: 271000, avg_rewards: 6.15\n",
      "td_loss: 0.10306143760681152\t q_values: 12.876241683959961\t step: 271500, avg_rewards: 6.42\n",
      "td_loss: 0.07685775309801102\t q_values: 13.084169387817383\t step: 272000, avg_rewards: 6.42\n",
      "td_loss: 0.0697062537074089\t q_values: 12.967754364013672\t step: 272500, avg_rewards: 6.42\n",
      "td_loss: 0.08329290896654129\t q_values: 12.958415985107422\t step: 273000, avg_rewards: 6.42\n",
      "td_loss: 0.09916567802429199\t q_values: 12.617116928100586\t step: 273500, avg_rewards: 6.54\n",
      "td_loss: 0.15248730778694153\t q_values: 12.740377426147461\t step: 274000, avg_rewards: 6.54\n",
      "td_loss: 0.11719480156898499\t q_values: 12.699711799621582\t step: 274500, avg_rewards: 6.54\n",
      "td_loss: 0.10032317042350769\t q_values: 12.846159934997559\t step: 275000, avg_rewards: 6.54\n",
      "td_loss: 0.06878209114074707\t q_values: 12.64975357055664\t step: 275500, avg_rewards: 6.76\n",
      "td_loss: 0.09645659476518631\t q_values: 12.725800514221191\t step: 276000, avg_rewards: 6.76\n",
      "td_loss: 0.09702907502651215\t q_values: 12.732593536376953\t step: 276500, avg_rewards: 6.76\n",
      "td_loss: 0.07464555650949478\t q_values: 12.888503074645996\t step: 277000, avg_rewards: 7.1\n",
      "td_loss: 0.10073845833539963\t q_values: 12.80104923248291\t step: 277500, avg_rewards: 7.1\n",
      "td_loss: 0.13680380582809448\t q_values: 12.754074096679688\t step: 278000, avg_rewards: 7.1\n",
      "td_loss: 0.12792140245437622\t q_values: 12.741141319274902\t step: 278500, avg_rewards: 7.1\n",
      "td_loss: 0.1313353329896927\t q_values: 12.885488510131836\t step: 279000, avg_rewards: 7.3\n",
      "td_loss: 0.08630223572254181\t q_values: 12.775348663330078\t step: 279500, avg_rewards: 7.3\n",
      "td_loss: 0.13524138927459717\t q_values: 12.756795883178711\t step: 280000, avg_rewards: 7.3\n",
      "td_loss: 0.07201151549816132\t q_values: 12.802720069885254\t step: 280500, avg_rewards: 7.65\n",
      "td_loss: 0.13549944758415222\t q_values: 12.640341758728027\t step: 281000, avg_rewards: 7.65\n",
      "td_loss: 0.08389824628829956\t q_values: 12.871886253356934\t step: 281500, avg_rewards: 7.65\n",
      "td_loss: 0.07952661067247391\t q_values: 12.860668182373047\t step: 282000, avg_rewards: 7.65\n",
      "td_loss: 0.12848511338233948\t q_values: 12.799100875854492\t step: 282500, avg_rewards: 7.93\n",
      "td_loss: 0.13168689608573914\t q_values: 12.72353458404541\t step: 283000, avg_rewards: 7.93\n",
      "td_loss: 0.10110391676425934\t q_values: 12.510292053222656\t step: 283500, avg_rewards: 7.93\n",
      "td_loss: 0.1381816565990448\t q_values: 12.428481101989746\t step: 284000, avg_rewards: 8.29\n",
      "td_loss: 0.11750615388154984\t q_values: 12.721284866333008\t step: 284500, avg_rewards: 8.29\n",
      "td_loss: 0.10126271843910217\t q_values: 12.582694053649902\t step: 285000, avg_rewards: 8.29\n",
      "td_loss: 0.11811600625514984\t q_values: 12.6620512008667\t step: 285500, avg_rewards: 8.29\n",
      "td_loss: 0.09783861041069031\t q_values: 12.714957237243652\t step: 286000, avg_rewards: 8.37\n",
      "td_loss: 0.12314795702695847\t q_values: 12.72879695892334\t step: 286500, avg_rewards: 8.37\n",
      "td_loss: 0.10443399101495743\t q_values: 12.725584030151367\t step: 287000, avg_rewards: 8.37\n",
      "td_loss: 0.10698403418064117\t q_values: 12.577402114868164\t step: 287500, avg_rewards: 8.37\n",
      "td_loss: 0.12722772359848022\t q_values: 12.610836029052734\t step: 288000, avg_rewards: 8.78\n",
      "td_loss: 0.10416997969150543\t q_values: 12.91671085357666\t step: 288500, avg_rewards: 8.78\n",
      "td_loss: 0.10873517394065857\t q_values: 12.765922546386719\t step: 289000, avg_rewards: 8.78\n",
      "td_loss: 0.09249184280633926\t q_values: 12.896333694458008\t step: 289500, avg_rewards: 9.11\n",
      "td_loss: 0.08440728485584259\t q_values: 12.632234573364258\t step: 290000, avg_rewards: 9.11\n",
      "td_loss: 0.10267496109008789\t q_values: 12.775582313537598\t step: 290500, avg_rewards: 9.11\n",
      "td_loss: 0.08589770644903183\t q_values: 12.818504333496094\t step: 291000, avg_rewards: 9.11\n",
      "td_loss: 0.08746933192014694\t q_values: 12.56468391418457\t step: 291500, avg_rewards: 9.37\n",
      "td_loss: 0.11773335933685303\t q_values: 12.641973495483398\t step: 292000, avg_rewards: 9.37\n",
      "td_loss: 0.09129512310028076\t q_values: 12.636676788330078\t step: 292500, avg_rewards: 9.37\n",
      "td_loss: 0.09491981565952301\t q_values: 12.614051818847656\t step: 293000, avg_rewards: 9.58\n",
      "td_loss: 0.098168283700943\t q_values: 12.187047004699707\t step: 293500, avg_rewards: 9.58\n",
      "td_loss: 0.09837006032466888\t q_values: 12.456708908081055\t step: 294000, avg_rewards: 9.58\n",
      "td_loss: 0.08804289251565933\t q_values: 12.672319412231445\t step: 294500, avg_rewards: 9.58\n",
      "td_loss: 0.10772611200809479\t q_values: 12.440385818481445\t step: 295000, avg_rewards: 9.75\n",
      "td_loss: 0.07147581875324249\t q_values: 12.180577278137207\t step: 295500, avg_rewards: 9.75\n",
      "td_loss: 0.10088871419429779\t q_values: 12.436759948730469\t step: 296000, avg_rewards: 9.75\n",
      "td_loss: 0.12746982276439667\t q_values: 12.297883987426758\t step: 296500, avg_rewards: 10.11\n",
      "td_loss: 0.14482112228870392\t q_values: 12.341377258300781\t step: 297000, avg_rewards: 10.11\n",
      "td_loss: 0.09745018929243088\t q_values: 12.4073486328125\t step: 297500, avg_rewards: 10.11\n",
      "td_loss: 0.0775393545627594\t q_values: 12.396946907043457\t step: 298000, avg_rewards: 10.11\n",
      "td_loss: 0.11646704375743866\t q_values: 12.257352828979492\t step: 298500, avg_rewards: 10.55\n",
      "td_loss: 0.11665435135364532\t q_values: 12.029855728149414\t step: 299000, avg_rewards: 10.55\n",
      "td_loss: 0.09772900491952896\t q_values: 12.180010795593262\t step: 299500, avg_rewards: 10.55\n",
      "td_loss: 0.05307764932513237\t q_values: 12.181305885314941\t step: 300000, avg_rewards: 10.55\n",
      "td_loss: 0.07186552882194519\t q_values: 12.316664695739746\t step: 300500, avg_rewards: 11.04\n",
      "td_loss: 0.09819892793893814\t q_values: 12.505976676940918\t step: 301000, avg_rewards: 11.04\n",
      "td_loss: 0.12692850828170776\t q_values: 12.343660354614258\t step: 301500, avg_rewards: 11.04\n",
      "td_loss: 0.07009571045637131\t q_values: 12.250425338745117\t step: 302000, avg_rewards: 11.25\n",
      "td_loss: 0.13193379342556\t q_values: 12.108392715454102\t step: 302500, avg_rewards: 11.25\n",
      "td_loss: 0.17122706770896912\t q_values: 12.061582565307617\t step: 303000, avg_rewards: 11.25\n",
      "td_loss: 0.10504446923732758\t q_values: 12.269058227539062\t step: 303500, avg_rewards: 11.25\n",
      "td_loss: 0.14078107476234436\t q_values: 12.194714546203613\t step: 304000, avg_rewards: 11.43\n",
      "td_loss: 0.08168360590934753\t q_values: 12.204972267150879\t step: 304500, avg_rewards: 11.43\n",
      "td_loss: 0.07707299292087555\t q_values: 12.229129791259766\t step: 305000, avg_rewards: 11.43\n",
      "td_loss: 0.11652674525976181\t q_values: 12.245166778564453\t step: 305500, avg_rewards: 11.41\n",
      "td_loss: 0.13784551620483398\t q_values: 12.122249603271484\t step: 306000, avg_rewards: 11.41\n",
      "td_loss: 0.14853200316429138\t q_values: 12.237747192382812\t step: 306500, avg_rewards: 11.41\n",
      "td_loss: 0.11754769086837769\t q_values: 12.265924453735352\t step: 307000, avg_rewards: 11.41\n",
      "td_loss: 0.1319817155599594\t q_values: 12.379315376281738\t step: 307500, avg_rewards: 11.8\n",
      "td_loss: 0.09817443788051605\t q_values: 12.35710334777832\t step: 308000, avg_rewards: 11.8\n",
      "td_loss: 0.09883326292037964\t q_values: 12.420220375061035\t step: 308500, avg_rewards: 11.8\n",
      "td_loss: 0.1487099826335907\t q_values: 12.349364280700684\t step: 309000, avg_rewards: 11.98\n",
      "td_loss: 0.08485065400600433\t q_values: 12.468801498413086\t step: 309500, avg_rewards: 11.98\n",
      "td_loss: 0.13107086718082428\t q_values: 12.497522354125977\t step: 310000, avg_rewards: 11.98\n",
      "td_loss: 0.12360863387584686\t q_values: 12.299314498901367\t step: 310500, avg_rewards: 11.98\n",
      "td_loss: 0.10712908208370209\t q_values: 12.139031410217285\t step: 311000, avg_rewards: 12.05\n",
      "td_loss: 0.1466061919927597\t q_values: 12.131280899047852\t step: 311500, avg_rewards: 12.05\n",
      "td_loss: 0.09180460125207901\t q_values: 12.353671073913574\t step: 312000, avg_rewards: 12.05\n",
      "td_loss: 0.12362302839756012\t q_values: 12.321022987365723\t step: 312500, avg_rewards: 12.05\n",
      "td_loss: 0.08552467823028564\t q_values: 12.071060180664062\t step: 313000, avg_rewards: 12.24\n",
      "td_loss: 0.16921477019786835\t q_values: 12.028718948364258\t step: 313500, avg_rewards: 12.24\n",
      "td_loss: 0.11738234013319016\t q_values: 12.154462814331055\t step: 314000, avg_rewards: 12.24\n",
      "td_loss: 0.12711037695407867\t q_values: 12.015328407287598\t step: 314500, avg_rewards: 12.37\n",
      "td_loss: 0.12008589506149292\t q_values: 11.773162841796875\t step: 315000, avg_rewards: 12.37\n",
      "td_loss: 0.1097216010093689\t q_values: 12.215568542480469\t step: 315500, avg_rewards: 12.37\n",
      "td_loss: 0.10296061635017395\t q_values: 12.055142402648926\t step: 316000, avg_rewards: 12.37\n",
      "td_loss: 0.09765702486038208\t q_values: 12.31495475769043\t step: 316500, avg_rewards: 12.56\n",
      "td_loss: 0.15148454904556274\t q_values: 12.002145767211914\t step: 317000, avg_rewards: 12.56\n",
      "td_loss: 0.14009174704551697\t q_values: 12.025827407836914\t step: 317500, avg_rewards: 12.56\n",
      "td_loss: 0.11330437660217285\t q_values: 11.863119125366211\t step: 318000, avg_rewards: 13.02\n",
      "td_loss: 0.10446784645318985\t q_values: 11.993587493896484\t step: 318500, avg_rewards: 13.02\n",
      "td_loss: 0.11335601657629013\t q_values: 11.776140213012695\t step: 319000, avg_rewards: 13.02\n",
      "td_loss: 0.07253886014223099\t q_values: 12.06869888305664\t step: 319500, avg_rewards: 13.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m     actions = env.action_space.sample()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     q_values = \u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     actions = torch.argmax(q_values, dim=\u001b[32m1\u001b[39m).cpu().numpy()\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(actions) == np.ndarray:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mQNetwork.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/activation.py:133\u001b[39m, in \u001b[36mReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/functional.py:1704\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1702\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "total_rewards = []\n",
    "total_reward = 0\n",
    "for step in range(total_timesteps):\n",
    "\n",
    "    # Epsilon greedy\n",
    "    if random.random() < epsilon:\n",
    "        actions = env.action_space.sample()\n",
    "        \n",
    "    else:\n",
    "\n",
    "        q_values = q_network(torch.tensor(obs, device=device, dtype=torch.float32).unsqueeze(dim=0))\n",
    "        actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "        \n",
    "    if type(actions) == np.ndarray:\n",
    "        actions = actions.item()\n",
    "\n",
    "    next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    total_reward += rewards\n",
    "\n",
    "    buffer.add(obs, actions, next_obs, rewards, terminations)\n",
    "    obs = next_obs\n",
    "\n",
    "    if terminations or truncations:\n",
    "        obs, _ = env.reset()\n",
    "        total_rewards.append(total_reward)\n",
    "        total_reward = 0\n",
    "\n",
    "    # Training.\n",
    "    if step > learning_starts:\n",
    "        if step % train_frequency == 0:\n",
    "            data = buffer.sample(batch_size)\n",
    "            buffer_obs, act, next_buffer_obs, rew, cont = data\n",
    "            with torch.no_grad():\n",
    "                target_max = target_network(next_buffer_obs).max(dim=1, keepdim=True)[0]\n",
    "                td_target = rew + gamma * target_max * cont\n",
    "            old_val = q_network(buffer_obs).gather(1, act)\n",
    "            loss = F.mse_loss(td_target, old_val)\n",
    "\n",
    "            if step % log_frequency == 0:\n",
    "                # wandb.log({\"td_loss\": loss.item(), \"q_values\": old_val.mean().item()}, step=step)\n",
    "                print('td_loss: {}\\t q_values: {}\\t step: {}, avg_rewards: {}'.format(loss.item(), old_val.mean().item(), step, np.mean(total_rewards[-100:])))\n",
    "            \n",
    "            # optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # update target network\n",
    "        if step % target_frequency == 0:\n",
    "            for target_param, param in zip(target_network.parameters(), q_network.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实验五练习\n",
    "\n",
    "1. 不修改超参和网络结构，将DQN算法修改为Double DQN\n",
    "2. 不修改超参，将DQN算法修改为Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
