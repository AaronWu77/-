{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1418d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "新建三个文件夹：data_t ，save,output\n",
    "准备数据, 参考https://www.zhihu.com/question/648983164/answer/1921290178963703022\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "data_dir=\"data_t\"\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join(data_dir, 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(data_dir, 'train.bin'))\n",
    "val_ids.tofile(os.path.join(data_dir, 'val.bin'))\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)\n",
    "\n",
    "# length of dataset in characters:  1115394\n",
    "# all the unique characters:\n",
    "#  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
    "# vocab size: 65\n",
    "# train has 1003854 tokens\n",
    "# val has 111540 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cea8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbfa8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration Parameters (Manually set instead of command-line parsing)\n",
    "out_dir = 'save'\n",
    "eval_interval = 250 \n",
    "log_interval = 10\n",
    "eval_iters = 200\n",
    "always_save_checkpoint = False\n",
    "wandb_log = False\n",
    "wandb_project = 'shakespeare-char'\n",
    "wandb_run_name = 'mini-gpt'\n",
    "dataset = ''\n",
    "gradient_accumulation_steps = 1\n",
    "batch_size = 64\n",
    "block_size = 256 \n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "learning_rate = 1e-3\n",
    "max_iters = 1000\n",
    "lr_decay_iters = 1000\n",
    "min_lr = 1e-4\n",
    "beta1=0.9\n",
    "beta2 = 0.95\n",
    "warmup_iters = 100\n",
    "device = 'cuda'  # Use CUDA (GPU) if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weight_decay = 1e-1\n",
    "best_val_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37146344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 29.94M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2591942/4054957035.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 30,031,872 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2591942/4054957035.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: loss 10.8135, time 375.54ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2591942/4054957035.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.6963, val loss 10.6917\n",
      "Iter 10: loss 9.2362, time 92.18ms\n",
      "Iter 20: loss 8.6765, time 86.26ms\n",
      "Iter 30: loss 7.9621, time 96.03ms\n",
      "Iter 40: loss 6.8436, time 81.80ms\n",
      "Iter 50: loss 5.4119, time 80.29ms\n",
      "Iter 60: loss 4.0301, time 90.75ms\n",
      "Iter 70: loss 3.2477, time 89.24ms\n",
      "Iter 80: loss 2.9289, time 87.63ms\n",
      "Iter 90: loss 2.7490, time 92.86ms\n",
      "Iter 100: loss 2.6312, time 93.15ms\n",
      "Iter 110: loss 2.5720, time 81.75ms\n",
      "Iter 120: loss 2.5692, time 85.85ms\n",
      "Iter 130: loss 2.5437, time 80.77ms\n",
      "Iter 140: loss 2.5360, time 90.12ms\n",
      "Iter 150: loss 2.5328, time 84.12ms\n",
      "Iter 160: loss 2.5345, time 80.69ms\n",
      "Iter 170: loss 2.5082, time 87.10ms\n",
      "Iter 180: loss 2.5011, time 84.97ms\n",
      "Iter 190: loss 2.4980, time 88.58ms\n",
      "Iter 200: loss 2.4936, time 78.88ms\n",
      "Iter 210: loss 2.4779, time 81.45ms\n",
      "Iter 220: loss 2.4671, time 91.30ms\n",
      "Iter 230: loss 2.4978, time 87.72ms\n",
      "Iter 240: loss 2.4628, time 91.73ms\n",
      "Iter 250: loss 2.4550, time 80.16ms\n",
      "step 250: train loss 2.4416, val loss 2.4507\n",
      "Iter 260: loss 2.4762, time 93.47ms\n",
      "Iter 270: loss 2.4652, time 88.56ms\n",
      "Iter 280: loss 2.4664, time 90.90ms\n",
      "Iter 290: loss 2.4412, time 85.40ms\n",
      "Iter 300: loss 2.4267, time 89.19ms\n",
      "Iter 310: loss 2.4392, time 87.82ms\n",
      "Iter 320: loss 2.4250, time 87.63ms\n",
      "Iter 330: loss 2.4245, time 93.28ms\n",
      "Iter 340: loss 2.4194, time 81.29ms\n",
      "Iter 350: loss 2.4052, time 87.24ms\n",
      "Iter 360: loss 2.4104, time 87.29ms\n",
      "Iter 370: loss 2.4230, time 87.92ms\n",
      "Iter 380: loss 2.3999, time 86.55ms\n",
      "Iter 390: loss 2.3888, time 81.74ms\n",
      "Iter 400: loss 2.3866, time 87.61ms\n",
      "Iter 410: loss 2.3863, time 96.11ms\n",
      "Iter 420: loss 2.3648, time 82.33ms\n",
      "Iter 430: loss 2.3768, time 92.13ms\n",
      "Iter 440: loss 2.3521, time 89.00ms\n",
      "Iter 450: loss 2.3482, time 90.42ms\n",
      "Iter 460: loss 2.3467, time 90.48ms\n",
      "Iter 470: loss 2.3025, time 95.28ms\n",
      "Iter 480: loss 2.3359, time 86.19ms\n",
      "Iter 490: loss 2.3136, time 86.88ms\n",
      "Iter 500: loss 2.3160, time 82.83ms\n",
      "step 500: train loss 2.2309, val loss 2.2821\n",
      "Iter 510: loss 2.2891, time 86.03ms\n",
      "Iter 520: loss 2.3133, time 90.62ms\n",
      "Iter 530: loss 2.2973, time 82.00ms\n",
      "Iter 540: loss 2.2805, time 92.96ms\n",
      "Iter 550: loss 2.2625, time 82.31ms\n",
      "Iter 560: loss 2.2604, time 82.93ms\n",
      "Iter 570: loss 2.2740, time 89.86ms\n",
      "Iter 580: loss 2.2596, time 82.90ms\n",
      "Iter 590: loss 2.2658, time 82.54ms\n",
      "Iter 600: loss 2.2118, time 85.45ms\n",
      "Iter 610: loss 2.2490, time 90.48ms\n",
      "Iter 620: loss 2.2242, time 90.76ms\n",
      "Iter 630: loss 2.2026, time 88.34ms\n",
      "Iter 640: loss 2.2093, time 87.44ms\n",
      "Iter 650: loss 2.2082, time 93.47ms\n",
      "Iter 660: loss 2.1945, time 82.10ms\n",
      "Iter 670: loss 2.2090, time 82.32ms\n",
      "Iter 680: loss 2.1645, time 81.90ms\n",
      "Iter 690: loss 2.1859, time 88.33ms\n",
      "Iter 700: loss 2.1757, time 93.13ms\n",
      "Iter 710: loss 2.1752, time 92.48ms\n",
      "Iter 720: loss 2.1571, time 94.52ms\n",
      "Iter 730: loss 2.1479, time 94.04ms\n",
      "Iter 740: loss 2.1674, time 89.01ms\n",
      "Iter 750: loss 2.1314, time 82.00ms\n",
      "step 750: train loss 2.0324, val loss 2.1270\n",
      "Iter 760: loss 2.1345, time 81.86ms\n",
      "Iter 770: loss 2.1115, time 82.02ms\n",
      "Iter 780: loss 2.1160, time 94.64ms\n",
      "Iter 790: loss 2.1198, time 88.18ms\n",
      "Iter 800: loss 2.1432, time 93.80ms\n",
      "Iter 810: loss 2.1051, time 88.67ms\n",
      "Iter 820: loss 2.1065, time 83.40ms\n",
      "Iter 830: loss 2.1411, time 92.78ms\n",
      "Iter 840: loss 2.1331, time 92.23ms\n",
      "Iter 850: loss 2.1254, time 91.77ms\n",
      "Iter 860: loss 2.0884, time 81.73ms\n",
      "Iter 870: loss 2.0806, time 93.20ms\n",
      "Iter 880: loss 2.0905, time 89.74ms\n",
      "Iter 890: loss 2.0894, time 82.74ms\n",
      "Iter 900: loss 2.0916, time 81.28ms\n",
      "Iter 910: loss 2.1096, time 82.35ms\n",
      "Iter 920: loss 2.1160, time 93.03ms\n",
      "Iter 930: loss 2.0516, time 92.72ms\n",
      "Iter 940: loss 2.0888, time 88.04ms\n",
      "Iter 950: loss 2.0551, time 87.42ms\n",
      "Iter 960: loss 2.0553, time 86.94ms\n",
      "Iter 970: loss 2.0743, time 84.76ms\n",
      "Iter 980: loss 2.0646, time 92.72ms\n",
      "Iter 990: loss 2.0330, time 82.61ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model configuration and model\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=False, vocab_size=50304, dropout=dropout)\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf).to(device)\n",
    "dtype = 'float16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float32'\n",
    "\n",
    "# Initialize a GradScaler if using mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = model.configure_optimizers( weight_decay, learning_rate,(beta1, beta2), device_type='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data loading function (as a replacement for the batch generator in the script)\n",
    "def get_batch(split):\n",
    "    data_dir = os.path.join('data_t', dataset)\n",
    "\n",
    "    data = np.memmap(os.path.join(data_dir, f'{split}.bin'), dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Learning rate decay function\n",
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it + 1) / (warmup_iters + 1)\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) \n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# Loss estimation (over both train and validation splits)\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Training loop\n",
    "t0 = time.time()\n",
    "iter_num = 0\n",
    "while iter_num < max_iters:\n",
    "    # Get new batch\n",
    "    X, Y = get_batch('train')\n",
    "    \n",
    "    # Learning rate for this iteration\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # Forward-backward pass with gradient accumulation\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        X, Y = get_batch('train')\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    # Step the optimizer\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Logging (print loss every log_interval)\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        print(f\"Iter {iter_num}: loss {loss.item():.4f}, time {dt*1000:.2f}ms\")\n",
    "    \n",
    "    # Evaluate and save checkpoint\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), os.path.join(out_dir, 'checkpoint.pt'))\n",
    "    \n",
    "    iter_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a09ab7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 29.94M\n"
     ]
    }
   ],
   "source": [
    "# Imports,推理\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import tiktoken\n",
    "from contextlib import nullcontext\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=False, vocab_size=50304, dropout=dropout)\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf).to(device)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration Parameters (Manually set instead of command-line parsing)\n",
    "init_from = 'resume'  # Can either be 'resume' or 'gpt2' (if you want to load a GPT-2 model)\n",
    "checkpoint_path = '/home/lichenglin/class/save/checkpoint.pt'  # Path to the checkpoint\n",
    "out_dir = './output'  # Directory for output (not used if init_from != 'resume')\n",
    "start = \"\\n\"  # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10  # number of samples to draw\n",
    "max_new_tokens = 500  # number of tokens generated in each sample\n",
    "temperature = 0.8  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'  # 'float32', 'bfloat16', 'float16'\n",
    "compile = False  # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Setup the device and seed\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a031ebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from /home/lichenglin/class/data_t/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model loading\n",
    "if init_from == 'resume':\n",
    "    # Load the model from a checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    state_dict=checkpoint\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # state_dict = checkpoint['model']\n",
    "    # unwanted_prefix = '_orig_mod.'\n",
    "    # for k, v in list(state_dict.items()):\n",
    "    #     if k.startswith(unwanted_prefix):\n",
    "    #         state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    # model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # Load from GPT-2 if desired\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# Encoding setup\n",
    "load_meta = False\n",
    "if init_from == 'resume':\n",
    "    meta_path = \"/home/lichenglin/class/data_t/meta.pkl\"\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "\n",
    "# If meta.pkl exists, load it, otherwise use GPT-2 encoding\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "# Encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# # Run generation\n",
    "# with torch.no_grad():\n",
    "#     with ctx:\n",
    "#         for k in range(num_samples):\n",
    "#             y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "#             print(decode(y[0].tolist()))\n",
    "#             print('---------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6857ae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And the the willd, hat lay be mad the bube to tanth ther\n",
      "Beat tand bar hiphe us him toth cedelass,\n",
      "Butw crut and not to hour he ownsploof inth bether.\n",
      "KING RICHARD II:\n",
      "I hain lat Heaid ove the the me nown iserans\n",
      "Welll no thave thu my de my soth thates hew y hour near y fopetelives\n",
      "And the the tounke o Winou w thithichs e ath dour wa cheshimeds poor ower ore\n",
      "To knom thrur af sorr igr tomef thin maleronts,\n",
      "Mad ared my o myord-bube!\n",
      "At y disar adst Wisith he mavein cour ayo tey Ire tof han t care.\n",
      "---------------\n",
      "\n",
      "Men pand knem sull of cont beest so a and\n",
      "The-sen man the thim tand the mand he win lies.\n",
      "\n",
      "IESTIO:\n",
      "So frepectsst, cas lom my ish he lop ints.\n",
      "\n",
      "LUCIO:\n",
      "A not lo to ime ther to o sink he thatimes thed his.\n",
      "\n",
      "FRRUCE:\n",
      "I you thald gicest he hand duche nee, to anor thitheat t agrerce\n",
      "And wam s sheve buml ne o thes wellll, thive,\n",
      "The ad isurde theim hingh ton inond Ceamey\n",
      "Tho that istius tthe nof the suthe wip ythere wube thang thers,\n",
      "Whe werellk t ono thy m wish bechtw thit ime to t thanker;\n",
      "The wen out\n",
      "---------------\n",
      "\n",
      "Mor thand thou hat sir fet the ange wher the o hid sprive\n",
      "tand the thow tho shergal of, mese things se tenes\n",
      "Singin ing tof ther Which thend chin beared\n",
      "He bence oIn pefor thein cand aseved of and\n",
      "The my lame at my brend sing bremeng town\n",
      "DUKE VINGHARGBRRD:\n",
      "Why hay willl whert forellls owh.\n",
      "The ourr thou jurake f thent Maurtht,\n",
      "Shat at bube t ith his; d anollt y donour the your henas bend sey o grone,\n",
      "Ond to this theab e then henave inde ngotong Thinerte y, delel gevenss.\n",
      "\n",
      "IELANT:\n",
      "Rer wholld wet\n",
      "---------------\n",
      "\n",
      "The she forke thou that mart sages soff the thous\n",
      "And scace: you pronet thiser sefl\n",
      "Sing strareve the course thon my Lary.\n",
      "\n",
      "KING RIV:\n",
      "Now.\n",
      "RWARD ICHARD II:\n",
      "Ay thar ark a give, the courm his,\n",
      "And as to not this se have blay nofend,\n",
      "My with legh fort that chreres chit, ofart, llinod m thee weat here winch arend owith the bus the therower lind he punict our owet thith our knong of ans y,\n",
      "Pry promy he griavenseng,\n",
      "But hat wench stho, the ha panforir brifons men'ed.\n",
      "What me suk whot whehe w willld wi\n",
      "---------------\n",
      "\n",
      "BOLANGE:\n",
      "And that his lome the fe corin,\n",
      "And I Gram high t fas to gane,\n",
      "And the epeerd of to her dour isen.\n",
      "POLES:\n",
      "But to cor fome I wenot?\n",
      "Bem not bsore of hat sis is ith shee\n",
      "Thip he nor ad you pthat thy cour thenthe\n",
      "And thise the preed tham y, surend theence!\n",
      "\n",
      "TMRURENE:\n",
      "I une whoulth las donience d hey mearce,\n",
      "\n",
      "Ther apptouonter alds bee y liss ofeve, hons may my shey,\n",
      "And the bunows brord sow thoou f mareraing s?\n",
      "The movence ome thim wom thais ty win trow h denolld f funore.\n",
      "\n",
      "\n",
      "HELLWINGBR:\n",
      "I b\n",
      "---------------\n",
      "\n",
      "\n",
      "MENARE:\n",
      "ADll, make the thy hou to god be.\n",
      "\n",
      "GROUCETENT:\n",
      "Ay, I and thou drast whe, my now.\n",
      "\n",
      "KING RICHARD II:\n",
      "She shat the: porirt thand be crump matheer\n",
      "Whel anes lour for bee theth me worseing oneclf.\n",
      "KING RICHAD II:\n",
      "Cliner, hind me yourr farker downces,\n",
      "And meath d bithit s onab here butht hangr wapowhthere s.\n",
      "Therg, yer thikng! ainght of myishth n imanceled.\n",
      "O to ndin t yof. thouy w himalld hast istht anre;\n",
      "Pend y myothat be crincengmene ty het byot my y,\n",
      "Tou ther soblliots s aringhten he ispt\n",
      "---------------\n",
      "\n",
      "Sher cons mand his broftite,\n",
      "Wich theset, che sing whirengs ince thin\n",
      "And lo the als se: cais ande: yor,\n",
      "I priar ut aid sto o the then not of dend\n",
      "And thry hasourd ahaver indy\n",
      "Fer:\n",
      "And I what not bes ill hal sand sle to co, whis kno yourd\n",
      "Lod gie hing thath at hup ancif orngsarss,\n",
      "\n",
      "And theat is the heave mbsth trow hey meve.\n",
      "What ses. how falll, theat sor'd theler owa wnd peeer ore thitur h.\n",
      "chy is Heth ma odod thank e yowen the anatht's pof herd the ore thathxp itoning then oo bbuse,\n",
      "St at o bu\n",
      "---------------\n",
      "\n",
      "ly nothou drealvionnte fer orstis bent.\n",
      "CARLIUS:\n",
      "You al ith coml gake and ther sar mant the\n",
      "To thee bowarede play be theve win thy lou,\n",
      "Ther that wild pin wath ame brodurp;\n",
      "What my tare poonions. I youref woll\n",
      "Musirst: of marast to soun the dof sortser imee.\n",
      "Prver:\n",
      "Thay te been stretis, there dofour sthe wipsttiours and!\n",
      "Agt cope, haverted to sprees orr thich\n",
      "The the phor angs thethe m wond thow here this coff iorchs, anche m shaburer iven o munas and me writ ofes sthew me ar d\n",
      "with with chim ow\n",
      "---------------\n",
      "\n",
      "GLER:\n",
      "So wit trer gran fry my mand, do bese and: tliny, know,\n",
      "And by godsvef trenome the finy soos,\n",
      "Of Rom the the sthise poce ton, sond not sw.\n",
      "DUKE VINCETI:\n",
      "A he your mon bory hou shater tut.\n",
      "\n",
      "KING VINCENCE:\n",
      "I my, ounk, sil my and rros therer the she wich chaw ham we beche ba ind ndusht beve\n",
      "Wird that inch atf oraul, whord beeas inght omghing\n",
      "I there sheld fove this is ingh torer thind seng htoom msherk\n",
      "Th Marde I thin dunow blit ond the hat atof urnge ut.\n",
      "Hort thru mard y srunou, ith, this st\n",
      "---------------\n",
      "\n",
      "QUETIN I His I I rogh:\n",
      "I now sre wener for to four the wa le\n",
      "He the stren to he thy imerseme?\n",
      "\n",
      "BRBUGHEND ORKE:\n",
      "Andt so sand the you getht me.\n",
      "\n",
      "SROKED VINCE:\n",
      "My en, may gastrte,\n",
      "\n",
      "MARGARD II:\n",
      "I brert thy she ou wen ring an ayour\n",
      "tow cong, dinll my with prose head in bofft\n",
      "And land-d why fy theau s areve athr mofe thighiness becetinde y.\n",
      "\n",
      "\n",
      "HERRYONRK:\n",
      "Whelt the ourd grosacy merds,\n",
      "\n",
      "Wecon LORDY OK:\n",
      "Rir sund shap ars sesint eith t demetulf.\n",
      "Sor, Then. Mush, ser thy itis shat'de anckes o,\n",
      "And buth thre\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
